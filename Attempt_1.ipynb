{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T01:38:23.914823Z",
     "start_time": "2024-10-09T01:38:21.915674Z"
    }
   },
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T02:05:57.952089Z",
     "start_time": "2024-10-09T02:05:57.866830Z"
    }
   },
   "source": [
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class CustomCityscapesDataset(Dataset):\n",
    "    def __init__(self, image_dir, label_dir, transform=None, load_color_labels=False):\n",
    "        self.image_dir = image_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.transform = transform\n",
    "        self.load_color_labels = load_color_labels  # Load both label types\n",
    "        self.images = []\n",
    "        self.labels_ids = []\n",
    "        self.labels_colors = []  # For color visualization\n",
    "\n",
    "        # List all image and label file pairs\n",
    "        for folder in os.listdir(image_dir):\n",
    "            folder_image_dir = os.path.join(image_dir, folder)\n",
    "            folder_label_dir = os.path.join(label_dir, folder)\n",
    "\n",
    "            if os.path.isdir(folder_image_dir) and os.path.isdir(folder_label_dir):\n",
    "                for image_file in os.listdir(folder_image_dir):\n",
    "                    if image_file.endswith(\"_leftImg8bit.jpg\"):\n",
    "                        # Corresponding label (ID-based)\n",
    "                        label_id_file = image_file.replace(\"_leftImg8bit.jpg\", \"_gtFine_labellevel3Ids.png\")\n",
    "                        # Corresponding label (color-coded)\n",
    "                        label_color_file = image_file.replace(\"_leftImg8bit.jpg\", \"_gtFine_labelColors.png\")\n",
    "\n",
    "                        # Full paths\n",
    "                        self.images.append(os.path.join(folder_image_dir, image_file))\n",
    "                        self.labels_ids.append(os.path.join(folder_label_dir, label_id_file))\n",
    "                        self.labels_colors.append(os.path.join(folder_label_dir, label_color_file))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.images[idx]\n",
    "        label_id_path = self.labels_ids[idx]\n",
    "        label_color_path = self.labels_colors[idx] if self.load_color_labels else None\n",
    "\n",
    "        # Open image and label\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        label_id = Image.open(label_id_path)\n",
    "        label_color = Image.open(label_color_path) if label_color_path else None\n",
    "\n",
    "        # Apply transforms to the image (e.g., resizing, normalization, etc.)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Convert label ID to tensor where each pixel is treated as a class ID\n",
    "        label_id = torch.tensor(np.array(label_id), dtype=torch.long)\n",
    "\n",
    "        # Optional: Load color label if requested\n",
    "        if label_color:\n",
    "            label_color = transforms.ToTensor()(label_color)  # Convert color label to a tensor\n",
    "\n",
    "        return (image, label_id, label_color) if label_color else (image, label_id)\n",
    "\n",
    "# Example usage:\n",
    "image_dir = \"D:/New folder/Inter_Bootcamp/dataset/train\"\n",
    "label_dir = \"D:/New folder/Inter_Bootcamp/dataset/labels\"\n",
    "\n",
    "# Define transforms for the input image\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),  # Resize as per your needs\n",
    "    transforms.ToTensor(),  # Convert to tensor and normalize\n",
    "])\n",
    "\n",
    "# If you want to load color labels for visualization, set `load_color_labels=True`\n",
    "dataset = CustomCityscapesDataset(image_dir, label_dir, transform=transform, load_color_labels=True)\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T02:05:58.049858Z",
     "start_time": "2024-10-09T02:05:58.046064Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 8  # Adjust this based on your GPU/CPU capability\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T02:05:59.444748Z",
     "start_time": "2024-10-09T02:05:59.438089Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "\n",
    "class CustomDatasetWithPolygons(CustomDataset):\n",
    "    def __getitem__(self, idx):\n",
    "        image, label = super().__getitem__(idx)\n",
    "        \n",
    "        # Loading corresponding polygons JSON\n",
    "        json_path = self.images[idx].replace(\"_leftImg8bit.jpg\", \"_gtFine_polygons.json\")\n",
    "        with open(json_path, 'r') as f:\n",
    "            polygons_data = json.load(f)\n",
    "\n",
    "        # You can process polygon data here\n",
    "\n",
    "        return image, label, polygons_data"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T02:28:27.609113Z",
     "start_time": "2024-10-09T02:28:25.726291Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torchvision.datasets import Cityscapes\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import json\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Custom Cityscapes Dataset\n",
    "class CustomCityscapesDataset(Dataset):\n",
    "    def __init__(self, image_dir, label_dir, transform=None, load_color_labels=False):\n",
    "        self.image_dir = image_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.transform = transform\n",
    "        self.load_color_labels = load_color_labels  # Load both label types\n",
    "        self.images = []\n",
    "        self.labels_ids = []\n",
    "        self.labels_colors = []  # For color visualization\n",
    "\n",
    "        # List all image and label file pairs\n",
    "        for folder in os.listdir(image_dir):\n",
    "            folder_image_dir = os.path.join(image_dir, folder)\n",
    "            folder_label_dir = os.path.join(label_dir, folder)\n",
    "\n",
    "            if os.path.isdir(folder_image_dir) and os.path.isdir(folder_label_dir):\n",
    "                for image_file in os.listdir(folder_image_dir):\n",
    "                    if image_file.endswith(\"_leftImg8bit.jpg\"):\n",
    "                        # Corresponding label (ID-based)\n",
    "                        label_id_file = image_file.replace(\"_leftImg8bit.jpg\", \"_gtFine_labellevel3Ids.png\")\n",
    "                        # Corresponding label (color-coded)\n",
    "                        label_color_file = image_file.replace(\"_leftImg8bit.jpg\", \"_gtFine_labelColors.png\")\n",
    "\n",
    "                        # Full paths\n",
    "                        self.images.append(os.path.join(folder_image_dir, image_file))\n",
    "                        self.labels_ids.append(os.path.join(folder_label_dir, label_id_file))\n",
    "                        self.labels_colors.append(os.path.join(folder_label_dir, label_color_file))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.images[idx]\n",
    "        label_id_path = self.labels_ids[idx]\n",
    "        label_color_path = self.labels_colors[idx] if self.load_color_labels else None\n",
    "\n",
    "        # Open image and label\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        label_id = Image.open(label_id_path)\n",
    "        label_color = Image.open(label_color_path) if label_color_path else None\n",
    "\n",
    "        # Apply transforms to the image (e.g., resizing, normalization, etc.)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Convert label ID to tensor where each pixel is treated as a class ID\n",
    "        label_id = torch.tensor(np.array(label_id), dtype=torch.long)\n",
    "\n",
    "        # Optional: Load color label if requested\n",
    "        if label_color:\n",
    "            label_color = transforms.ToTensor()(label_color)  # Convert color label to a tensor\n",
    "\n",
    "        return (image, label_id, label_color) if label_color else (image, label_id)\n",
    "\n",
    "# Dataset with polygons\n",
    "class CustomDatasetWithPolygons(CustomCityscapesDataset):\n",
    "    def __getitem__(self, idx):\n",
    "        image, label = super().__getitem__(idx)\n",
    "        \n",
    "        # Loading corresponding polygons JSON\n",
    "        json_path = self.images[idx].replace(\"_leftImg8bit.jpg\", \"_gtFine_polygons.json\")\n",
    "        with open(json_path, 'r') as f:\n",
    "            polygons_data = json.load(f)\n",
    "\n",
    "        # You can process polygon data here\n",
    "        return image, label, polygons_data\n",
    "\n",
    "# Albumentations for data augmentation\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "transform = A.Compose([\n",
    "    A.Resize(256, 512),\n",
    "    A.HorizontalFlip(),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "# Utility functions for label encoding/decoding\n",
    "ignore_index = 255\n",
    "valid_classes = [ignore_index, 7, 8, 11, 12, 13, 17, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 31, 32, 33]\n",
    "n_classes = len(valid_classes)\n",
    "\n",
    "class_map = dict(zip(valid_classes, range(n_classes)))\n",
    "colors = [\n",
    "    [0, 0, 0], [128, 64, 128], [244, 35, 232], [70, 70, 70], [102, 102, 156], \n",
    "    [190, 153, 153], [153, 153, 153], [250, 170, 30], [220, 220, 0], [107, 142, 35], \n",
    "    [152, 251, 152], [0, 130, 180], [220, 20, 60], [255, 0, 0], [0, 0, 142], \n",
    "    [0, 0, 70], [0, 60, 100], [0, 80, 100], [0, 0, 230], [119, 11, 32]\n",
    "]\n",
    "label_colours = dict(zip(range(n_classes), colors))\n",
    "\n",
    "def encode_segmap(mask):\n",
    "    for _validc in valid_classes:\n",
    "        mask[mask == _validc] = class_map[_validc]\n",
    "    return mask\n",
    "\n",
    "def decode_segmap(temp):\n",
    "    r, g, b = temp.copy(), temp.copy(), temp.copy()\n",
    "    for l in range(n_classes):\n",
    "        r[temp == l], g[temp == l], b[temp == l] = label_colours[l]\n",
    "    rgb = np.stack([r, g, b], axis=2)\n",
    "    return rgb / 255.0\n",
    "\n",
    "# Model definition\n",
    "import segmentation_models_pytorch as smp\n",
    "from pytorch_lightning import LightningModule, Trainer\n",
    "import torchmetrics\n",
    "\n",
    "class OurModel(LightningModule):\n",
    "    def __init__(self):\n",
    "        super(OurModel, self).__init__()\n",
    "        self.model = smp.Unet(\n",
    "            encoder_name=\"resnet34\", \n",
    "            encoder_weights=\"imagenet\", \n",
    "            in_channels=3, \n",
    "            classes=n_classes\n",
    "        )\n",
    "        self.lr = 1e-3\n",
    "        self.criterion = smp.losses.DiceLoss(mode='multiclass')\n",
    "        self.metrics = torchmetrics.IoU(num_classes=n_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(self.parameters(), lr=self.lr)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        img, seg = batch\n",
    "        output = self(img)\n",
    "        seg = encode_segmap(seg)\n",
    "        loss = self.criterion(output, seg)\n",
    "        iou = self.metrics(output, seg)\n",
    "        self.log('train_loss', loss, on_epoch=True)\n",
    "        self.log('train_iou', iou, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        img, seg = batch\n",
    "        output = self(img)\n",
    "        seg = encode_segmap(seg)\n",
    "        loss = self.criterion(output, seg)\n",
    "        iou = self.metrics(output, seg)\n",
    "        self.log('val_loss', loss, on_epoch=True)\n",
    "        self.log('val_iou', iou, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "# Dataloaders\n",
    "train_dataset = CustomCityscapesDataset(\n",
    "    image_dir=\"D:/New folder/Inter_Bootcamp/dataset/train\",\n",
    "    label_dir=\"D:/New folder/Inter_Bootcamp/dataset/labels\",\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "val_dataset = CustomCityscapesDataset(\n",
    "    image_dir=\"D:/New folder/Inter_Bootcamp/dataset/val\",\n",
    "    label_dir=\"D:/New folder/Inter_Bootcamp/dataset/labels\",\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# Training\n",
    "checkpoint_callback = ModelCheckpoint(monitor='val_loss', save_top_k=1)\n",
    "trainer = Trainer(\n",
    "    max_epochs=200, \n",
    "    gpus=1 if torch.cuda.is_available() else 0, \n",
    "    precision=16, \n",
    "    callbacks=[checkpoint_callback]\n",
    ")\n",
    "model = OurModel()\n",
    "trainer.fit(model, train_loader, val_loader)"
   ],
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'D:/New folder/Inter_Bootcamp/dataset/val'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[19], line 168\u001B[0m\n\u001B[0;32m    161\u001B[0m \u001B[38;5;66;03m# Dataloaders\u001B[39;00m\n\u001B[0;32m    162\u001B[0m train_dataset \u001B[38;5;241m=\u001B[39m CustomCityscapesDataset(\n\u001B[0;32m    163\u001B[0m     image_dir\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mD:/New folder/Inter_Bootcamp/dataset/train\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    164\u001B[0m     label_dir\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mD:/New folder/Inter_Bootcamp/dataset/labels\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    165\u001B[0m     transform\u001B[38;5;241m=\u001B[39mtransform\n\u001B[0;32m    166\u001B[0m )\n\u001B[1;32m--> 168\u001B[0m val_dataset \u001B[38;5;241m=\u001B[39m \u001B[43mCustomCityscapesDataset\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    169\u001B[0m \u001B[43m    \u001B[49m\u001B[43mimage_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mD:/New folder/Inter_Bootcamp/dataset/val\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    170\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlabel_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mD:/New folder/Inter_Bootcamp/dataset/labels\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    171\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtransform\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtransform\u001B[49m\n\u001B[0;32m    172\u001B[0m \u001B[43m)\u001B[49m\n\u001B[0;32m    174\u001B[0m train_loader \u001B[38;5;241m=\u001B[39m DataLoader(train_dataset, batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m16\u001B[39m, shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m    175\u001B[0m val_loader \u001B[38;5;241m=\u001B[39m DataLoader(val_dataset, batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m16\u001B[39m, shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "Cell \u001B[1;32mIn[19], line 24\u001B[0m, in \u001B[0;36mCustomCityscapesDataset.__init__\u001B[1;34m(self, image_dir, label_dir, transform, load_color_labels)\u001B[0m\n\u001B[0;32m     21\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlabels_colors \u001B[38;5;241m=\u001B[39m []  \u001B[38;5;66;03m# For color visualization\u001B[39;00m\n\u001B[0;32m     23\u001B[0m \u001B[38;5;66;03m# List all image and label file pairs\u001B[39;00m\n\u001B[1;32m---> 24\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m folder \u001B[38;5;129;01min\u001B[39;00m os\u001B[38;5;241m.\u001B[39mlistdir(image_dir):\n\u001B[0;32m     25\u001B[0m     folder_image_dir \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(image_dir, folder)\n\u001B[0;32m     26\u001B[0m     folder_label_dir \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(label_dir, folder)\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [WinError 3] The system cannot find the path specified: 'D:/New folder/Inter_Bootcamp/dataset/val'"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T02:40:12.993418Z",
     "start_time": "2024-10-09T02:40:08.642662Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import json\n",
    "import segmentation_models_pytorch as smp\n",
    "import torchmetrics\n",
    "from pytorch_lightning import LightningModule, Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from torchmetrics import JaccardIndex\n",
    "\n",
    "# Custom dataset class\n",
    "class CustomCityscapesDataset(Dataset):\n",
    "    def __init__(self, image_dir, label_dir, transform=None, load_color_labels=False):\n",
    "        self.image_dir = image_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.transform = transform\n",
    "        self.load_color_labels = load_color_labels\n",
    "        self.images = []\n",
    "        self.labels_ids = []\n",
    "        self.labels_colors = []\n",
    "\n",
    "        for folder in os.listdir(image_dir):\n",
    "            folder_image_dir = os.path.join(image_dir, folder)\n",
    "            folder_label_dir = os.path.join(label_dir, folder)\n",
    "\n",
    "            if os.path.isdir(folder_image_dir) and os.path.isdir(folder_label_dir):\n",
    "                for image_file in os.listdir(folder_image_dir):\n",
    "                    if image_file.endswith(\"_leftImg8bit.jpg\"):\n",
    "                        label_id_file = image_file.replace(\"_leftImg8bit.jpg\", \"_gtFine_labellevel3Ids.png\")\n",
    "                        label_color_file = image_file.replace(\"_leftImg8bit.jpg\", \"_gtFine_labelColors.png\")\n",
    "\n",
    "                        self.images.append(os.path.join(folder_image_dir, image_file))\n",
    "                        self.labels_ids.append(os.path.join(folder_label_dir, label_id_file))\n",
    "                        self.labels_colors.append(os.path.join(folder_label_dir, label_color_file))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.images[idx]\n",
    "        label_id_path = self.labels_ids[idx]\n",
    "        label_color_path = self.labels_colors[idx] if self.load_color_labels else None\n",
    "\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        label_id = Image.open(label_id_path)\n",
    "        label_color = Image.open(label_color_path) if label_color_path else None\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        label_id = torch.tensor(np.array(label_id), dtype=torch.long)\n",
    "\n",
    "        if label_color:\n",
    "            label_color = transforms.ToTensor()(label_color)\n",
    "\n",
    "        return (image, label_id, label_color) if label_color else (image, label_id)\n",
    "\n",
    "\n",
    "# Split the train dataset into train/val\n",
    "def split_train_val(dataset, val_split=0.2):\n",
    "    val_size = int(len(dataset) * val_split)\n",
    "    train_size = len(dataset) - val_size\n",
    "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "    return train_dataset, val_dataset\n",
    "\n",
    "\n",
    "# Model class\n",
    "class OurModel(LightningModule):\n",
    "    def __init__(self, train_dataset, val_dataset, n_classes, transform):\n",
    "        super(OurModel, self).__init__()\n",
    "        self.layer = smp.Unet(\n",
    "            encoder_name=\"resnet34\",  # choose encoder\n",
    "            encoder_weights=\"imagenet\",  # use imagenet pre-trained weights\n",
    "            in_channels=3,  # model input channels (RGB images)\n",
    "            classes=n_classes  # model output channels (number of classes in your dataset)\n",
    "        )\n",
    "        self.lr = 1e-3\n",
    "        self.batch_size = 16\n",
    "        self.criterion = smp.losses.DiceLoss(mode='multiclass')\n",
    "        self.metrics = torchmetrics.JaccardIndex(task='multiclass',num_classes=n_classes)\n",
    "        self.train_class = train_dataset\n",
    "        self.val_class = val_dataset\n",
    "\n",
    "    def process(self, image, segment):\n",
    "        out = self(image)\n",
    "        loss = self.criterion(out, segment.long())\n",
    "        iou = self.metrics(out, segment)\n",
    "        return loss, iou\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layer(x)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(self.parameters(), lr=self.lr)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_class, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        image, segment = batch\n",
    "        loss, iou = self.process(image, segment)\n",
    "        self.log('train_loss', loss, on_epoch=True)\n",
    "        self.log('train_iou', iou, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_class, batch_size=self.batch_size, shuffle=False)\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        image, segment = batch\n",
    "        loss, iou = self.process(image, segment)\n",
    "        self.log('val_loss', loss, on_epoch=True)\n",
    "        self.log('val_iou', iou, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "\n",
    "# Training the model\n",
    "image_dir = \"D:/New folder/Inter_Bootcamp/dataset/train\"\n",
    "label_dir = \"D:/New folder/Inter_Bootcamp/dataset/labels\"\n",
    "\n",
    "# Define transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Create the dataset\n",
    "dataset = CustomCityscapesDataset(image_dir, label_dir, transform=transform)\n",
    "\n",
    "# Split into train and val\n",
    "train_dataset, val_dataset = split_train_val(dataset)\n",
    "\n",
    "n_classes = 40  # Define your number of classes based on your dataset\n",
    "\n",
    "# Initialize model\n",
    "model = OurModel(train_dataset, val_dataset, n_classes, transform)\n",
    "\n",
    "# Checkpoints\n",
    "checkpoint_callback = ModelCheckpoint(monitor='val_loss', dirpath='checkpoints', filename='model', save_last=True)\n",
    "\n",
    "# Training\n",
    "trainer = Trainer(max_epochs=50, accelerator=\"auto\", callbacks=[checkpoint_callback])\n",
    "trainer.fit(model)\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\prakh\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\logger_connector\\logger_connector.py:75: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "\n",
      "  | Name      | Type                   | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | layer     | Unet                   | 24.4 M | train\n",
      "1 | criterion | DiceLoss               | 0      | train\n",
      "2 | metrics   | MulticlassJaccardIndex | 0      | train\n",
      "-------------------------------------------------------------\n",
      "24.4 M    Trainable params\n",
      "0         Non-trainable params\n",
      "24.4 M    Total params\n",
      "97.768    Total estimated model params size (MB)\n",
      "190       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4fc79f7fe85d463389156d304628a41a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prakh\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Class values must be smaller than num_classes.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[22], line 147\u001B[0m\n\u001B[0;32m    145\u001B[0m \u001B[38;5;66;03m# Training\u001B[39;00m\n\u001B[0;32m    146\u001B[0m trainer \u001B[38;5;241m=\u001B[39m Trainer(max_epochs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m50\u001B[39m, accelerator\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mauto\u001B[39m\u001B[38;5;124m\"\u001B[39m, callbacks\u001B[38;5;241m=\u001B[39m[checkpoint_callback])\n\u001B[1;32m--> 147\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:538\u001B[0m, in \u001B[0;36mTrainer.fit\u001B[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001B[0m\n\u001B[0;32m    536\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mstatus \u001B[38;5;241m=\u001B[39m TrainerStatus\u001B[38;5;241m.\u001B[39mRUNNING\n\u001B[0;32m    537\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m--> 538\u001B[0m \u001B[43mcall\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_and_handle_interrupt\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    539\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit_impl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_dataloaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_dataloaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdatamodule\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mckpt_path\u001B[49m\n\u001B[0;32m    540\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py:47\u001B[0m, in \u001B[0;36m_call_and_handle_interrupt\u001B[1;34m(trainer, trainer_fn, *args, **kwargs)\u001B[0m\n\u001B[0;32m     45\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m trainer\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39mlauncher \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m     46\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m trainer\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39mlauncher\u001B[38;5;241m.\u001B[39mlaunch(trainer_fn, \u001B[38;5;241m*\u001B[39margs, trainer\u001B[38;5;241m=\u001B[39mtrainer, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m---> 47\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtrainer_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     49\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m _TunerExitException:\n\u001B[0;32m     50\u001B[0m     _call_teardown_hook(trainer)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:574\u001B[0m, in \u001B[0;36mTrainer._fit_impl\u001B[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001B[0m\n\u001B[0;32m    567\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mfn \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    568\u001B[0m ckpt_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_checkpoint_connector\u001B[38;5;241m.\u001B[39m_select_ckpt_path(\n\u001B[0;32m    569\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mfn,\n\u001B[0;32m    570\u001B[0m     ckpt_path,\n\u001B[0;32m    571\u001B[0m     model_provided\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[0;32m    572\u001B[0m     model_connected\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlightning_module \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    573\u001B[0m )\n\u001B[1;32m--> 574\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mckpt_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mckpt_path\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    576\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mstopped\n\u001B[0;32m    577\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:981\u001B[0m, in \u001B[0;36mTrainer._run\u001B[1;34m(self, model, ckpt_path)\u001B[0m\n\u001B[0;32m    976\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_signal_connector\u001B[38;5;241m.\u001B[39mregister_signal_handlers()\n\u001B[0;32m    978\u001B[0m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n\u001B[0;32m    979\u001B[0m \u001B[38;5;66;03m# RUN THE TRAINER\u001B[39;00m\n\u001B[0;32m    980\u001B[0m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n\u001B[1;32m--> 981\u001B[0m results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run_stage\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    983\u001B[0m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n\u001B[0;32m    984\u001B[0m \u001B[38;5;66;03m# POST-Training CLEAN UP\u001B[39;00m\n\u001B[0;32m    985\u001B[0m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n\u001B[0;32m    986\u001B[0m log\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m: trainer tearing down\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1023\u001B[0m, in \u001B[0;36mTrainer._run_stage\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1021\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining:\n\u001B[0;32m   1022\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m isolate_rng():\n\u001B[1;32m-> 1023\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run_sanity_check\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1024\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mautograd\u001B[38;5;241m.\u001B[39mset_detect_anomaly(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_detect_anomaly):\n\u001B[0;32m   1025\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfit_loop\u001B[38;5;241m.\u001B[39mrun()\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1052\u001B[0m, in \u001B[0;36mTrainer._run_sanity_check\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1049\u001B[0m call\u001B[38;5;241m.\u001B[39m_call_callback_hooks(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mon_sanity_check_start\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m   1051\u001B[0m \u001B[38;5;66;03m# run eval step\u001B[39;00m\n\u001B[1;32m-> 1052\u001B[0m \u001B[43mval_loop\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1054\u001B[0m call\u001B[38;5;241m.\u001B[39m_call_callback_hooks(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mon_sanity_check_end\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m   1056\u001B[0m \u001B[38;5;66;03m# reset logger connector\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\loops\\utilities.py:178\u001B[0m, in \u001B[0;36m_no_grad_context.<locals>._decorator\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    176\u001B[0m     context_manager \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mno_grad\n\u001B[0;32m    177\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m context_manager():\n\u001B[1;32m--> 178\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mloop_run\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\loops\\evaluation_loop.py:135\u001B[0m, in \u001B[0;36m_EvaluationLoop.run\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    133\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbatch_progress\u001B[38;5;241m.\u001B[39mis_last_batch \u001B[38;5;241m=\u001B[39m data_fetcher\u001B[38;5;241m.\u001B[39mdone\n\u001B[0;32m    134\u001B[0m     \u001B[38;5;66;03m# run step hooks\u001B[39;00m\n\u001B[1;32m--> 135\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_evaluation_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdataloader_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdataloader_iter\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    136\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m:\n\u001B[0;32m    137\u001B[0m     \u001B[38;5;66;03m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001B[39;00m\n\u001B[0;32m    138\u001B[0m     \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\loops\\evaluation_loop.py:396\u001B[0m, in \u001B[0;36m_EvaluationLoop._evaluation_step\u001B[1;34m(self, batch, batch_idx, dataloader_idx, dataloader_iter)\u001B[0m\n\u001B[0;32m    390\u001B[0m hook_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtest_step\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m trainer\u001B[38;5;241m.\u001B[39mtesting \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvalidation_step\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    391\u001B[0m step_args \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m    392\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_build_step_args_from_hook_kwargs(hook_kwargs, hook_name)\n\u001B[0;32m    393\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m using_dataloader_iter\n\u001B[0;32m    394\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m (dataloader_iter,)\n\u001B[0;32m    395\u001B[0m )\n\u001B[1;32m--> 396\u001B[0m output \u001B[38;5;241m=\u001B[39m \u001B[43mcall\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_strategy_hook\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrainer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhook_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mstep_args\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    398\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbatch_progress\u001B[38;5;241m.\u001B[39mincrement_processed()\n\u001B[0;32m    400\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m using_dataloader_iter:\n\u001B[0;32m    401\u001B[0m     \u001B[38;5;66;03m# update the hook kwargs now that the step method might have consumed the iterator\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py:319\u001B[0m, in \u001B[0;36m_call_strategy_hook\u001B[1;34m(trainer, hook_name, *args, **kwargs)\u001B[0m\n\u001B[0;32m    316\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    318\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m trainer\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mprofile(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m[Strategy]\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtrainer\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mhook_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m--> 319\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    321\u001B[0m \u001B[38;5;66;03m# restore current_fx when nested context\u001B[39;00m\n\u001B[0;32m    322\u001B[0m pl_module\u001B[38;5;241m.\u001B[39m_current_fx_name \u001B[38;5;241m=\u001B[39m prev_fx_name\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\strategies\\strategy.py:411\u001B[0m, in \u001B[0;36mStrategy.validation_step\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    409\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlightning_module:\n\u001B[0;32m    410\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_redirection(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlightning_module, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvalidation_step\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m--> 411\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlightning_module\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvalidation_step\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[22], line 115\u001B[0m, in \u001B[0;36mOurModel.validation_step\u001B[1;34m(self, batch, batch_idx)\u001B[0m\n\u001B[0;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mvalidation_step\u001B[39m(\u001B[38;5;28mself\u001B[39m, batch, batch_idx):\n\u001B[0;32m    114\u001B[0m     image, segment \u001B[38;5;241m=\u001B[39m batch\n\u001B[1;32m--> 115\u001B[0m     loss, iou \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mprocess\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimage\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msegment\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    116\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlog(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mval_loss\u001B[39m\u001B[38;5;124m'\u001B[39m, loss, on_epoch\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m    117\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlog(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mval_iou\u001B[39m\u001B[38;5;124m'\u001B[39m, iou, on_epoch\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "Cell \u001B[1;32mIn[22], line 90\u001B[0m, in \u001B[0;36mOurModel.process\u001B[1;34m(self, image, segment)\u001B[0m\n\u001B[0;32m     88\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mprocess\u001B[39m(\u001B[38;5;28mself\u001B[39m, image, segment):\n\u001B[0;32m     89\u001B[0m     out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m(image)\n\u001B[1;32m---> 90\u001B[0m     loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcriterion\u001B[49m\u001B[43m(\u001B[49m\u001B[43mout\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msegment\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlong\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     91\u001B[0m     iou \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmetrics(out, segment)\n\u001B[0;32m     92\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m loss, iou\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\segmentation_models_pytorch\\losses\\dice.py:97\u001B[0m, in \u001B[0;36mDiceLoss.forward\u001B[1;34m(self, y_pred, y_true)\u001B[0m\n\u001B[0;32m     95\u001B[0m         y_true \u001B[38;5;241m=\u001B[39m y_true\u001B[38;5;241m.\u001B[39mpermute(\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m1\u001B[39m) \u001B[38;5;241m*\u001B[39m mask\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m1\u001B[39m)  \u001B[38;5;66;03m# N, C, H*W\u001B[39;00m\n\u001B[0;32m     96\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m---> 97\u001B[0m         y_true \u001B[38;5;241m=\u001B[39m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mone_hot\u001B[49m\u001B[43m(\u001B[49m\u001B[43my_true\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_classes\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# N,H*W -> N,H*W, C\u001B[39;00m\n\u001B[0;32m     98\u001B[0m         y_true \u001B[38;5;241m=\u001B[39m y_true\u001B[38;5;241m.\u001B[39mpermute(\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m1\u001B[39m)  \u001B[38;5;66;03m# N, C, H*W\u001B[39;00m\n\u001B[0;32m    100\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmode \u001B[38;5;241m==\u001B[39m MULTILABEL_MODE:\n",
      "\u001B[1;31mRuntimeError\u001B[0m: Class values must be smaller than num_classes."
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
