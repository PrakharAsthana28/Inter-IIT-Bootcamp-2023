{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-12T10:52:46.675162Z",
     "start_time": "2024-10-12T10:52:19.497254Z"
    }
   },
   "source": "!pip install transformers",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.45.2-py3-none-any.whl.metadata (44 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\prakh\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\prakh\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from transformers) (0.25.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\prakh\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\prakh\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\prakh\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2024.9.11-cp312-cp312-win_amd64.whl.metadata (41 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\prakh\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\prakh\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from transformers) (0.4.5)\n",
      "Collecting tokenizers<0.21,>=0.20 (from transformers)\n",
      "  Downloading tokenizers-0.20.1-cp312-none-win_amd64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\prakh\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\prakh\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\prakh\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.11.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\prakh\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\prakh\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\prakh\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\prakh\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\prakh\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from requests->transformers) (2024.8.30)\n",
      "Downloading transformers-4.45.2-py3-none-any.whl (9.9 MB)\n",
      "   ---------------------------------------- 0.0/9.9 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/9.9 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.5/9.9 MB 882.6 kB/s eta 0:00:11\n",
      "   -- ------------------------------------- 0.5/9.9 MB 882.6 kB/s eta 0:00:11\n",
      "   ---- ----------------------------------- 1.0/9.9 MB 1.2 MB/s eta 0:00:08\n",
      "   ------- -------------------------------- 1.8/9.9 MB 1.7 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 2.4/9.9 MB 1.8 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 2.6/9.9 MB 1.8 MB/s eta 0:00:04\n",
      "   ------------- -------------------------- 3.4/9.9 MB 2.0 MB/s eta 0:00:04\n",
      "   --------------- ------------------------ 3.9/9.9 MB 2.1 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 4.5/9.9 MB 2.1 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 5.0/9.9 MB 2.2 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 5.8/9.9 MB 2.3 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 6.6/9.9 MB 2.4 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 7.3/9.9 MB 2.4 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 7.9/9.9 MB 2.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 8.9/9.9 MB 2.6 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 9.4/9.9 MB 2.6 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 9.4/9.9 MB 2.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.9/9.9 MB 2.5 MB/s eta 0:00:00\n",
      "Downloading regex-2024.9.11-cp312-cp312-win_amd64.whl (273 kB)\n",
      "Downloading tokenizers-0.20.1-cp312-none-win_amd64.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   -------- ------------------------------- 0.5/2.4 MB 2.4 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 0.8/2.4 MB 2.2 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 1.6/2.4 MB 2.6 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 2.1/2.4 MB 2.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.4/2.4 MB 2.6 MB/s eta 0:00:00\n",
      "Installing collected packages: regex, tokenizers, transformers\n",
      "Successfully installed regex-2024.9.11 tokenizers-0.20.1 transformers-4.45.2\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T10:55:17.122741Z",
     "start_time": "2024-10-12T10:55:15.324270Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import SegformerForSemanticSegmentation, SegformerFeatureExtractor\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define number of classes for your segmentation task (example: 3 classes)\n",
    "num_classes = 40\n",
    "\n",
    "# Load a pretrained SegFormer model (B0 version is a good starting point)\n",
    "model = SegformerForSemanticSegmentation.from_pretrained(\"nvidia/segformer-b0-finetuned-ade-512-512\", num_labels=num_classes, ignore_mismatched_sizes=True)\n",
    "\n",
    "# Load feature extractor to process input images\n",
    "feature_extractor = SegformerFeatureExtractor.from_pretrained(\"nvidia/segformer-b0-finetuned-ade-512-512\")"
   ],
   "id": "fedef93db8888939",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/segformer-b0-finetuned-ade-512-512 and are newly initialized because the shapes did not match:\n",
      "- decode_head.classifier.bias: found shape torch.Size([150]) in the checkpoint and torch.Size([40]) in the model instantiated\n",
      "- decode_head.classifier.weight: found shape torch.Size([150, 256, 1, 1]) in the checkpoint and torch.Size([40, 256, 1, 1]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\prakh\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\transformers\\models\\segformer\\feature_extraction_segformer.py:28: FutureWarning: The class SegformerFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use SegformerImageProcessor instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\prakh\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\transformers\\utils\\deprecation.py:165: UserWarning: The following named arguments are not valid for `SegformerFeatureExtractor.__init__` and were ignored: 'feature_extractor_type'\n",
      "  return func(*args, **kwargs)\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T10:57:48.263794Z",
     "start_time": "2024-10-12T10:57:48.151646Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "\n",
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, image_folder, label_folder, transform=None, feature_extractor=None):\n",
    "        self.image_folder = image_folder\n",
    "        self.label_folder = label_folder\n",
    "        self.image_files = []\n",
    "        self.feature_extractor = feature_extractor  # Add feature extractor\n",
    "        \n",
    "        for batch_folder in os.listdir(image_folder):\n",
    "            batch_path = os.path.join(image_folder, batch_folder)\n",
    "            if os.path.isdir(batch_path):\n",
    "                self.image_files += [os.path.join(batch_path, f) for f in os.listdir(batch_path) if f.endswith('_leftImg8bit.jpg')]\n",
    "        \n",
    "        if not self.image_files:\n",
    "            raise ValueError(\"No images found in the image folder!\")\n",
    "\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_files[idx]\n",
    "        img_name = os.path.basename(img_path)\n",
    "        batch_name = os.path.basename(os.path.dirname(img_path))\n",
    "\n",
    "        label_path = os.path.join(self.label_folder, batch_name, f\"{img_name.split('_leftImg8bit')[0]}_gtFine_labelColors.png\")\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        label = Image.open(label_path).convert(\"L\")  # Convert label to grayscale\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            label = self.transform(label)\n",
    "\n",
    "        # Apply feature extractor\n",
    "        inputs = self.feature_extractor(images=image, return_tensors=\"pt\")\n",
    "        \n",
    "        return inputs['pixel_values'].squeeze(), torch.tensor(label)\n",
    "\n",
    "# Update transform to match model input requirements\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((512, 512)),  # SegFormer expects 512x512 input\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_folder = 'D:/New folder/Inter_Bootcamp/dataset/train'\n",
    "labels_folder = 'D:/New folder/Inter_Bootcamp/dataset/labels'\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = SegmentationDataset(train_folder, labels_folder, transform=transform, feature_extractor=feature_extractor)\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n"
   ],
   "id": "db3b9c30baf4485d",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T11:05:34.631318Z",
     "start_time": "2024-10-12T11:05:31.753668Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install tqdm",
   "id": "825ca8587c32772a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in c:\\users\\prakh\\anaconda3\\envs\\deeplearning\\lib\\site-packages (4.66.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\prakh\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from tqdm) (0.4.6)\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T11:06:46.141062Z",
     "start_time": "2024-10-12T11:06:02.341629Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "import os\n",
    "import json\n",
    "import segmentation_models_pytorch as smp\n",
    "import torchmetrics.functional as tmf\n",
    "from torchmetrics import JaccardIndex \n",
    "from pytorch_lightning import LightningModule, Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Optimizer and loss function\n",
    "optimizer = optim.AdamW(model.parameters(), lr=5e-5)\n",
    "criterion = torch.nn.CrossEntropyLoss()  # Suitable for multi-class segmentation tasks\n",
    "\n",
    "# Fine-tuning loop\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Training loop with tqdm for progress bar\n",
    "num_epochs = 10\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    # Wrap the dataloader with tqdm for the progress bar\n",
    "    progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\", unit=\"batch\")\n",
    "    \n",
    "    for images, labels in progress_bar:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Remove channel dimension from labels (squeeze to make shape [batch_size, height, width])\n",
    "        labels = labels.squeeze(1).long()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images).logits  # Outputs shape: (batch_size, num_classes, 128, 128)\n",
    "\n",
    "        # Upsample the outputs to match target size (batch_size, num_classes, 512, 512)\n",
    "        outputs = F.interpolate(outputs, size=labels.shape[1:], mode='bilinear', align_corners=False)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update progress bar with current loss\n",
    "        progress_bar.set_postfix(loss=total_loss / len(progress_bar))\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(dataloader)}\")\n"
   ],
   "id": "2fe5f65bb3fb5b76",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|          | 0/1759 [00:00<?, ?batch/s]C:\\Users\\prakh\\AppData\\Local\\Temp\\ipykernel_11136\\161639473.py:41: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return inputs['pixel_values'].squeeze(), torch.tensor(label)\n",
      "Epoch 1/10:   2%|▏         | 34/1759 [00:42<36:20,  1.26s/batch, loss=0.0423]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[17], line 35\u001B[0m\n\u001B[0;32m     32\u001B[0m \u001B[38;5;66;03m# Wrap the dataloader with tqdm for the progress bar\u001B[39;00m\n\u001B[0;32m     33\u001B[0m progress_bar \u001B[38;5;241m=\u001B[39m tqdm(dataloader, desc\u001B[38;5;241m=\u001B[39m\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEpoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnum_epochs\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m, unit\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbatch\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m---> 35\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m images, labels \u001B[38;5;129;01min\u001B[39;00m progress_bar:\n\u001B[0;32m     36\u001B[0m     images, labels \u001B[38;5;241m=\u001B[39m images\u001B[38;5;241m.\u001B[39mto(device), labels\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m     38\u001B[0m     \u001B[38;5;66;03m# Remove channel dimension from labels (squeeze to make shape [batch_size, height, width])\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\tqdm\\std.py:1181\u001B[0m, in \u001B[0;36mtqdm.__iter__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1178\u001B[0m time \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_time\n\u001B[0;32m   1180\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 1181\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m obj \u001B[38;5;129;01min\u001B[39;00m iterable:\n\u001B[0;32m   1182\u001B[0m         \u001B[38;5;28;01myield\u001B[39;00m obj\n\u001B[0;32m   1183\u001B[0m         \u001B[38;5;66;03m# Update and possibly print the progressbar.\u001B[39;00m\n\u001B[0;32m   1184\u001B[0m         \u001B[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    627\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    628\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[0;32m    629\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[1;32m--> 630\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_data()\n\u001B[0;32m    631\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    632\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    633\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    634\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:673\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    671\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    672\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m--> 673\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_fetcher\u001B[38;5;241m.\u001B[39mfetch(index)  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m    674\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[0;32m    675\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[1;34m(self, possibly_batched_index)\u001B[0m\n\u001B[0;32m     50\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[0;32m     51\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m---> 52\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[0;32m     53\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     54\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "Cell \u001B[1;32mIn[9], line 39\u001B[0m, in \u001B[0;36mSegmentationDataset.__getitem__\u001B[1;34m(self, idx)\u001B[0m\n\u001B[0;32m     36\u001B[0m     label \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransform(label)\n\u001B[0;32m     38\u001B[0m \u001B[38;5;66;03m# Apply feature extractor\u001B[39;00m\n\u001B[1;32m---> 39\u001B[0m inputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfeature_extractor(images\u001B[38;5;241m=\u001B[39mimage, return_tensors\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpt\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     41\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m inputs[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpixel_values\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39msqueeze(), torch\u001B[38;5;241m.\u001B[39mtensor(label)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\transformers\\models\\segformer\\image_processing_segformer.py:303\u001B[0m, in \u001B[0;36mSegformerImageProcessor.__call__\u001B[1;34m(self, images, segmentation_maps, **kwargs)\u001B[0m\n\u001B[0;32m    296\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, images, segmentation_maps\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m    297\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    298\u001B[0m \u001B[38;5;124;03m    Preprocesses a batch of images and optionally segmentation maps.\u001B[39;00m\n\u001B[0;32m    299\u001B[0m \n\u001B[0;32m    300\u001B[0m \u001B[38;5;124;03m    Overrides the `__call__` method of the `Preprocessor` class so that both images and segmentation maps can be\u001B[39;00m\n\u001B[0;32m    301\u001B[0m \u001B[38;5;124;03m    passed in as positional arguments.\u001B[39;00m\n\u001B[0;32m    302\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 303\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__call__\u001B[39m(images, segmentation_maps\u001B[38;5;241m=\u001B[39msegmentation_maps, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\transformers\\image_processing_utils.py:41\u001B[0m, in \u001B[0;36mBaseImageProcessor.__call__\u001B[1;34m(self, images, **kwargs)\u001B[0m\n\u001B[0;32m     39\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, images, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m BatchFeature:\n\u001B[0;32m     40\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Preprocess an image or a batch of images.\"\"\"\u001B[39;00m\n\u001B[1;32m---> 41\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpreprocess(images, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\transformers\\utils\\deprecation.py:165\u001B[0m, in \u001B[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    161\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m minimum_action \u001B[38;5;129;01min\u001B[39;00m (Action\u001B[38;5;241m.\u001B[39mNOTIFY, Action\u001B[38;5;241m.\u001B[39mNOTIFY_ALWAYS):\n\u001B[0;32m    162\u001B[0m     \u001B[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001B[39;00m\n\u001B[0;32m    163\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(message, \u001B[38;5;167;01mFutureWarning\u001B[39;00m, stacklevel\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m)\n\u001B[1;32m--> 165\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\transformers\\utils\\generic.py:852\u001B[0m, in \u001B[0;36mfilter_out_non_signature_kwargs.<locals>.decorator.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    843\u001B[0m         cls_prefix \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    845\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[0;32m    846\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe following named arguments are not valid for `\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcls_prefix\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m`\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    847\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m and were ignored: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00minvalid_kwargs_names\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    848\u001B[0m         \u001B[38;5;167;01mUserWarning\u001B[39;00m,\n\u001B[0;32m    849\u001B[0m         stacklevel\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m,\n\u001B[0;32m    850\u001B[0m     )\n\u001B[1;32m--> 852\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mvalid_kwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\transformers\\models\\segformer\\image_processing_segformer.py:404\u001B[0m, in \u001B[0;36mSegformerImageProcessor.preprocess\u001B[1;34m(self, images, segmentation_maps, do_resize, size, resample, do_rescale, rescale_factor, do_normalize, image_mean, image_std, do_reduce_labels, return_tensors, data_format, input_data_format)\u001B[0m\n\u001B[0;32m    388\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    389\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInvalid image type. Must be of type PIL.Image.Image, numpy.ndarray, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    390\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtorch.Tensor, tf.Tensor or jax.ndarray.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    391\u001B[0m     )\n\u001B[0;32m    392\u001B[0m validate_preprocess_arguments(\n\u001B[0;32m    393\u001B[0m     do_rescale\u001B[38;5;241m=\u001B[39mdo_rescale,\n\u001B[0;32m    394\u001B[0m     rescale_factor\u001B[38;5;241m=\u001B[39mrescale_factor,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    400\u001B[0m     resample\u001B[38;5;241m=\u001B[39mresample,\n\u001B[0;32m    401\u001B[0m )\n\u001B[0;32m    403\u001B[0m images \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m--> 404\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_preprocess_image(\n\u001B[0;32m    405\u001B[0m         image\u001B[38;5;241m=\u001B[39mimg,\n\u001B[0;32m    406\u001B[0m         do_resize\u001B[38;5;241m=\u001B[39mdo_resize,\n\u001B[0;32m    407\u001B[0m         resample\u001B[38;5;241m=\u001B[39mresample,\n\u001B[0;32m    408\u001B[0m         size\u001B[38;5;241m=\u001B[39msize,\n\u001B[0;32m    409\u001B[0m         do_rescale\u001B[38;5;241m=\u001B[39mdo_rescale,\n\u001B[0;32m    410\u001B[0m         rescale_factor\u001B[38;5;241m=\u001B[39mrescale_factor,\n\u001B[0;32m    411\u001B[0m         do_normalize\u001B[38;5;241m=\u001B[39mdo_normalize,\n\u001B[0;32m    412\u001B[0m         image_mean\u001B[38;5;241m=\u001B[39mimage_mean,\n\u001B[0;32m    413\u001B[0m         image_std\u001B[38;5;241m=\u001B[39mimage_std,\n\u001B[0;32m    414\u001B[0m         data_format\u001B[38;5;241m=\u001B[39mdata_format,\n\u001B[0;32m    415\u001B[0m         input_data_format\u001B[38;5;241m=\u001B[39minput_data_format,\n\u001B[0;32m    416\u001B[0m     )\n\u001B[0;32m    417\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m img \u001B[38;5;129;01min\u001B[39;00m images\n\u001B[0;32m    418\u001B[0m ]\n\u001B[0;32m    420\u001B[0m data \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpixel_values\u001B[39m\u001B[38;5;124m\"\u001B[39m: images}\n\u001B[0;32m    422\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m segmentation_maps \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\transformers\\models\\segformer\\image_processing_segformer.py:243\u001B[0m, in \u001B[0;36mSegformerImageProcessor._preprocess_image\u001B[1;34m(self, image, do_resize, size, resample, do_rescale, rescale_factor, do_normalize, image_mean, image_std, data_format, input_data_format)\u001B[0m\n\u001B[0;32m    241\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m input_data_format \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    242\u001B[0m     input_data_format \u001B[38;5;241m=\u001B[39m infer_channel_dimension_format(image)\n\u001B[1;32m--> 243\u001B[0m image \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_preprocess(\n\u001B[0;32m    244\u001B[0m     image\u001B[38;5;241m=\u001B[39mimage,\n\u001B[0;32m    245\u001B[0m     do_reduce_labels\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m    246\u001B[0m     do_resize\u001B[38;5;241m=\u001B[39mdo_resize,\n\u001B[0;32m    247\u001B[0m     size\u001B[38;5;241m=\u001B[39msize,\n\u001B[0;32m    248\u001B[0m     resample\u001B[38;5;241m=\u001B[39mresample,\n\u001B[0;32m    249\u001B[0m     do_rescale\u001B[38;5;241m=\u001B[39mdo_rescale,\n\u001B[0;32m    250\u001B[0m     rescale_factor\u001B[38;5;241m=\u001B[39mrescale_factor,\n\u001B[0;32m    251\u001B[0m     do_normalize\u001B[38;5;241m=\u001B[39mdo_normalize,\n\u001B[0;32m    252\u001B[0m     image_mean\u001B[38;5;241m=\u001B[39mimage_mean,\n\u001B[0;32m    253\u001B[0m     image_std\u001B[38;5;241m=\u001B[39mimage_std,\n\u001B[0;32m    254\u001B[0m     input_data_format\u001B[38;5;241m=\u001B[39minput_data_format,\n\u001B[0;32m    255\u001B[0m )\n\u001B[0;32m    256\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m data_format \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    257\u001B[0m     image \u001B[38;5;241m=\u001B[39m to_channel_dimension_format(image, data_format, input_channel_dim\u001B[38;5;241m=\u001B[39minput_data_format)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\transformers\\models\\segformer\\image_processing_segformer.py:209\u001B[0m, in \u001B[0;36mSegformerImageProcessor._preprocess\u001B[1;34m(self, image, do_reduce_labels, do_resize, do_rescale, do_normalize, size, resample, rescale_factor, image_mean, image_std, input_data_format)\u001B[0m\n\u001B[0;32m    206\u001B[0m     image \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreduce_label(image)\n\u001B[0;32m    208\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m do_resize:\n\u001B[1;32m--> 209\u001B[0m     image \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mresize(image\u001B[38;5;241m=\u001B[39mimage, size\u001B[38;5;241m=\u001B[39msize, resample\u001B[38;5;241m=\u001B[39mresample, input_data_format\u001B[38;5;241m=\u001B[39minput_data_format)\n\u001B[0;32m    211\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m do_rescale:\n\u001B[0;32m    212\u001B[0m     image \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrescale(image\u001B[38;5;241m=\u001B[39mimage, scale\u001B[38;5;241m=\u001B[39mrescale_factor, input_data_format\u001B[38;5;241m=\u001B[39minput_data_format)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\transformers\\models\\segformer\\image_processing_segformer.py:173\u001B[0m, in \u001B[0;36mSegformerImageProcessor.resize\u001B[1;34m(self, image, size, resample, data_format, input_data_format, **kwargs)\u001B[0m\n\u001B[0;32m    171\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe `size` dictionary must contain the keys `height` and `width`. Got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00msize\u001B[38;5;241m.\u001B[39mkeys()\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    172\u001B[0m output_size \u001B[38;5;241m=\u001B[39m (size[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mheight\u001B[39m\u001B[38;5;124m\"\u001B[39m], size[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwidth\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[1;32m--> 173\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m resize(\n\u001B[0;32m    174\u001B[0m     image,\n\u001B[0;32m    175\u001B[0m     size\u001B[38;5;241m=\u001B[39moutput_size,\n\u001B[0;32m    176\u001B[0m     resample\u001B[38;5;241m=\u001B[39mresample,\n\u001B[0;32m    177\u001B[0m     data_format\u001B[38;5;241m=\u001B[39mdata_format,\n\u001B[0;32m    178\u001B[0m     input_data_format\u001B[38;5;241m=\u001B[39minput_data_format,\n\u001B[0;32m    179\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m    180\u001B[0m )\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\transformers\\image_transforms.py:332\u001B[0m, in \u001B[0;36mresize\u001B[1;34m(image, size, resample, reducing_gap, data_format, return_numpy, input_data_format)\u001B[0m\n\u001B[0;32m    330\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(image, PIL\u001B[38;5;241m.\u001B[39mImage\u001B[38;5;241m.\u001B[39mImage):\n\u001B[0;32m    331\u001B[0m     do_rescale \u001B[38;5;241m=\u001B[39m _rescale_for_pil_conversion(image)\n\u001B[1;32m--> 332\u001B[0m     image \u001B[38;5;241m=\u001B[39m to_pil_image(image, do_rescale\u001B[38;5;241m=\u001B[39mdo_rescale, input_data_format\u001B[38;5;241m=\u001B[39minput_data_format)\n\u001B[0;32m    333\u001B[0m height, width \u001B[38;5;241m=\u001B[39m size\n\u001B[0;32m    334\u001B[0m \u001B[38;5;66;03m# PIL images are in the format (width, height)\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\transformers\\image_transforms.py:210\u001B[0m, in \u001B[0;36mto_pil_image\u001B[1;34m(image, do_rescale, input_data_format)\u001B[0m\n\u001B[0;32m    207\u001B[0m     image \u001B[38;5;241m=\u001B[39m rescale(image, \u001B[38;5;241m255\u001B[39m)\n\u001B[0;32m    209\u001B[0m image \u001B[38;5;241m=\u001B[39m image\u001B[38;5;241m.\u001B[39mastype(np\u001B[38;5;241m.\u001B[39muint8)\n\u001B[1;32m--> 210\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m PIL\u001B[38;5;241m.\u001B[39mImage\u001B[38;5;241m.\u001B[39mfromarray(image)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\PIL\\Image.py:3304\u001B[0m, in \u001B[0;36mfromarray\u001B[1;34m(obj, mode)\u001B[0m\n\u001B[0;32m   3301\u001B[0m         msg \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mstrides\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m requires either tobytes() or tostring()\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   3302\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(msg)\n\u001B[1;32m-> 3304\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m frombuffer(mode, size, obj, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mraw\u001B[39m\u001B[38;5;124m\"\u001B[39m, rawmode, \u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\PIL\\Image.py:3206\u001B[0m, in \u001B[0;36mfrombuffer\u001B[1;34m(mode, size, data, decoder_name, *args)\u001B[0m\n\u001B[0;32m   3203\u001B[0m         im\u001B[38;5;241m.\u001B[39mreadonly \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m   3204\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m im\n\u001B[1;32m-> 3206\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m frombytes(mode, size, data, decoder_name, args)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\PIL\\Image.py:3137\u001B[0m, in \u001B[0;36mfrombytes\u001B[1;34m(mode, size, data, decoder_name, *args)\u001B[0m\n\u001B[0;32m   3112\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   3113\u001B[0m \u001B[38;5;124;03mCreates a copy of an image memory from pixel data in a buffer.\u001B[39;00m\n\u001B[0;32m   3114\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   3132\u001B[0m \u001B[38;5;124;03m:returns: An :py:class:`~PIL.Image.Image` object.\u001B[39;00m\n\u001B[0;32m   3133\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   3135\u001B[0m _check_size(size)\n\u001B[1;32m-> 3137\u001B[0m im \u001B[38;5;241m=\u001B[39m new(mode, size)\n\u001B[0;32m   3138\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m im\u001B[38;5;241m.\u001B[39mwidth \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m im\u001B[38;5;241m.\u001B[39mheight \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m   3139\u001B[0m     decoder_args: Any \u001B[38;5;241m=\u001B[39m args\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\PIL\\Image.py:3102\u001B[0m, in \u001B[0;36mnew\u001B[1;34m(mode, size, color)\u001B[0m\n\u001B[0;32m   3100\u001B[0m         im\u001B[38;5;241m.\u001B[39mpalette \u001B[38;5;241m=\u001B[39m ImagePalette\u001B[38;5;241m.\u001B[39mImagePalette()\n\u001B[0;32m   3101\u001B[0m         color \u001B[38;5;241m=\u001B[39m im\u001B[38;5;241m.\u001B[39mpalette\u001B[38;5;241m.\u001B[39mgetcolor(color_ints)\n\u001B[1;32m-> 3102\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m im\u001B[38;5;241m.\u001B[39m_new(core\u001B[38;5;241m.\u001B[39mfill(mode, size, color))\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "2dfd8ab17973b3f7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
