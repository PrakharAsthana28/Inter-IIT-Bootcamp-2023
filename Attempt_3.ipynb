{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-10T20:39:14.744339Z",
     "start_time": "2024-10-10T20:39:14.719108Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def preprocess_image(image_path, size=(256, 256)):\n",
    "    image = cv2.imread(image_path)\n",
    "    resized_image = cv2.resize(image, size)\n",
    "    normalized_image = resized_image / 255.0  # Normalizing to [0, 1]\n",
    "    return normalized_image\n",
    "\n",
    "def preprocess_colored_mask(mask_path, size=(256, 256)):\n",
    "    mask = cv2.imread(mask_path)  # Load the mask as a color image\n",
    "    resized_mask = cv2.resize(mask, size)\n",
    "    return resized_mask\n",
    "\n",
    "def load_and_preprocess_data(train_folder, labels_folder, size=(256, 256)):\n",
    "    images = []\n",
    "    masks = []\n",
    "    level_segments = []\n",
    "    polygons = []\n",
    "\n",
    "    for batch_folder in os.listdir(train_folder):\n",
    "        batch_path = os.path.join(train_folder, batch_folder)\n",
    "        for file_name in os.listdir(batch_path):\n",
    "            if file_name.endswith(\"_leftImg8bit.png\"):\n",
    "                image_path = os.path.join(batch_path, file_name)\n",
    "                mask_path = os.path.join(labels_folder, f\"{file_name.split('_leftImg8bit')[0]}_gtFine_labelColors.png\")\n",
    "                level_segment_path = os.path.join(labels_folder, f\"{file_name.split('_leftImg8bit')[0]}_gtFine_labellevel3Ids.png\")\n",
    "                polygon_path = os.path.join(labels_folder, f\"{file_name.split('_leftImg8bit')[0]}_gtFine_polygons.json\")\n",
    "\n",
    "                images.append(preprocess_image(image_path, size))\n",
    "                masks.append(preprocess_colored_mask(mask_path, size))\n",
    "\n",
    "                # Load level-labeled segments and polygon data\n",
    "                level_segment = cv2.imread(level_segment_path, cv2.IMREAD_GRAYSCALE)\n",
    "                polygons.append(polygon_path)  # Store the path for later use\n",
    "\n",
    "                level_segments.append(level_segment)\n",
    "\n",
    "    return np.array(images), np.array(masks), np.array(level_segments), polygons\n",
    "\n",
    "train_folder = 'D:/New folder/Inter_Bootcamp/dataset/train'\n",
    "labels_folder = 'D:/New folder/Inter_Bootcamp/dataset/labels'\n",
    "images, masks, level_segments, polygons = load_and_preprocess_data(train_folder, labels_folder)\n",
    "\n",
    "# Note: Process the polygons (JSON) during training/inference as needed.\n"
   ],
   "id": "18e83b9125ec8e5c",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-10T20:39:37.572858Z",
     "start_time": "2024-10-10T20:39:14.855035Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "import os\n",
    "import json\n",
    "\n",
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, image_folder, label_folder, transform=None):\n",
    "        self.image_folder = image_folder\n",
    "        self.label_folder = label_folder\n",
    "        self.image_files = []\n",
    "        \n",
    "        for batch_folder in os.listdir(image_folder):\n",
    "            batch_path = os.path.join(image_folder, batch_folder)\n",
    "            if os.path.isdir(batch_path):\n",
    "                self.image_files += [os.path.join(batch_path, f) for f in os.listdir(batch_path) if f.endswith('_leftImg8bit.jpg')]\n",
    "        \n",
    "        if not self.image_files:\n",
    "            raise ValueError(\"No images found in the image folder!\")\n",
    "        \n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_files[idx]\n",
    "        img_name = os.path.basename(img_path)\n",
    "        batch_name = os.path.basename(os.path.dirname(img_path))\n",
    "\n",
    "        label_path = os.path.join(self.label_folder, batch_name, f\"{img_name.split('_leftImg8bit')[0]}_gtFine_labelColors.png\")\n",
    "        level_path = os.path.join(self.label_folder, batch_name, f\"{img_name.split('_leftImg8bit')[0]}_gtFine_labellevel3Ids.png\")\n",
    "        polygon_path = os.path.join(self.label_folder, batch_name, f\"{img_name.split('_leftImg8bit')[0]}_gtFine_polygons.json\")\n",
    "\n",
    "        image = Image.open(img_path)\n",
    "        label = Image.open(label_path)\n",
    "        level_segment = Image.open(level_path)\n",
    "        \n",
    "        with open(polygon_path, 'r') as file:\n",
    "            polygons = json.load(file)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            label = self.transform(label)\n",
    "            level_segment = self.transform(level_segment)\n",
    "        \n",
    "        # Debugging prints\n",
    "        print(f\"Image size: {image.size()}\")\n",
    "        print(f\"Label size: {label.size()}\")\n",
    "        print(f\"Level segment size: {level_segment.size()}\")\n",
    "\n",
    "        return image, label, level_segment, polygons\n",
    "\n",
    "# Custom collate function\n",
    "def custom_collate(batch):\n",
    "    images, labels, level_segments, polygons = zip(*batch)\n",
    "    images = torch.stack(images)\n",
    "    labels = torch.stack(labels)\n",
    "    level_segments = torch.stack(level_segments)\n",
    "    return images, labels, level_segments, polygons\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    #transforms.Resize((256, 256)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Create dataset\n",
    "train_folder = 'D:/New folder/Inter_Bootcamp/dataset/train'\n",
    "labels_folder = 'D:/New folder/Inter_Bootcamp/dataset/labels'\n",
    "dataset = SegmentationDataset(train_folder, labels_folder, transform=transform)\n",
    "\n",
    "# Create data loader with custom collate function\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=custom_collate)\n",
    "\n",
    "# Example usage\n",
    "for images, labels, level_segments, polygons in dataloader:\n",
    "    print(f\"Loaded batch with {images.size(0)} images.\")\n",
    "    break\n"
   ],
   "id": "e47db20f1e4db6e6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image size: torch.Size([3, 1080, 1920])\n",
      "Label size: torch.Size([4, 1080, 1920])\n",
      "Level segment size: torch.Size([1, 1080, 1920])\n",
      "Image size: torch.Size([3, 1080, 1920])\n",
      "Label size: torch.Size([4, 1080, 1920])\n",
      "Level segment size: torch.Size([1, 1080, 1920])\n",
      "Image size: torch.Size([3, 1080, 1920])\n",
      "Label size: torch.Size([4, 1080, 1920])\n",
      "Level segment size: torch.Size([1, 1080, 1920])\n",
      "Image size: torch.Size([3, 1080, 1920])\n",
      "Label size: torch.Size([4, 1080, 1920])\n",
      "Level segment size: torch.Size([1, 1080, 1920])\n",
      "Image size: torch.Size([3, 1080, 1920])\n",
      "Label size: torch.Size([4, 1080, 1920])\n",
      "Level segment size: torch.Size([1, 1080, 1920])\n",
      "Image size: torch.Size([3, 1080, 1920])\n",
      "Label size: torch.Size([4, 1080, 1920])\n",
      "Level segment size: torch.Size([1, 1080, 1920])\n",
      "Image size: torch.Size([3, 1080, 1920])\n",
      "Label size: torch.Size([4, 1080, 1920])\n",
      "Level segment size: torch.Size([1, 1080, 1920])\n",
      "Image size: torch.Size([3, 1080, 1920])\n",
      "Label size: torch.Size([4, 1080, 1920])\n",
      "Level segment size: torch.Size([1, 1080, 1920])\n",
      "Image size: torch.Size([3, 1080, 1920])\n",
      "Label size: torch.Size([4, 1080, 1920])\n",
      "Level segment size: torch.Size([1, 1080, 1920])\n",
      "Image size: torch.Size([3, 1080, 1920])\n",
      "Label size: torch.Size([4, 1080, 1920])\n",
      "Level segment size: torch.Size([1, 1080, 1920])\n",
      "Image size: torch.Size([3, 1080, 1920])\n",
      "Label size: torch.Size([4, 1080, 1920])\n",
      "Level segment size: torch.Size([1, 1080, 1920])\n",
      "Image size: torch.Size([3, 1080, 1920])\n",
      "Label size: torch.Size([4, 1080, 1920])\n",
      "Level segment size: torch.Size([1, 1080, 1920])\n",
      "Image size: torch.Size([3, 1080, 1920])\n",
      "Label size: torch.Size([4, 1080, 1920])\n",
      "Level segment size: torch.Size([1, 1080, 1920])\n",
      "Image size: torch.Size([3, 1080, 1920])\n",
      "Label size: torch.Size([4, 1080, 1920])\n",
      "Level segment size: torch.Size([1, 1080, 1920])\n",
      "Image size: torch.Size([3, 1080, 1920])\n",
      "Label size: torch.Size([4, 1080, 1920])\n",
      "Level segment size: torch.Size([1, 1080, 1920])\n",
      "Image size: torch.Size([3, 1080, 1920])\n",
      "Label size: torch.Size([4, 1080, 1920])\n",
      "Level segment size: torch.Size([1, 1080, 1920])\n",
      "Image size: torch.Size([3, 1080, 1920])\n",
      "Label size: torch.Size([4, 1080, 1920])\n",
      "Level segment size: torch.Size([1, 1080, 1920])\n",
      "Image size: torch.Size([3, 1080, 1920])\n",
      "Label size: torch.Size([4, 1080, 1920])\n",
      "Level segment size: torch.Size([1, 1080, 1920])\n",
      "Image size: torch.Size([3, 1080, 1920])\n",
      "Label size: torch.Size([4, 1080, 1920])\n",
      "Level segment size: torch.Size([1, 1080, 1920])\n",
      "Image size: torch.Size([3, 1080, 1920])\n",
      "Label size: torch.Size([4, 1080, 1920])\n",
      "Level segment size: torch.Size([1, 1080, 1920])\n",
      "Image size: torch.Size([3, 1080, 1920])\n",
      "Label size: torch.Size([4, 1080, 1920])\n",
      "Level segment size: torch.Size([1, 1080, 1920])\n",
      "Image size: torch.Size([3, 1080, 1920])\n",
      "Label size: torch.Size([4, 1080, 1920])\n",
      "Level segment size: torch.Size([1, 1080, 1920])\n",
      "Image size: torch.Size([3, 1080, 1920])\n",
      "Label size: torch.Size([4, 1080, 1920])\n",
      "Level segment size: torch.Size([1, 1080, 1920])\n",
      "Image size: torch.Size([3, 1080, 1920])\n",
      "Label size: torch.Size([4, 1080, 1920])\n",
      "Level segment size: torch.Size([1, 1080, 1920])\n",
      "Image size: torch.Size([3, 1080, 1920])\n",
      "Label size: torch.Size([4, 1080, 1920])\n",
      "Level segment size: torch.Size([1, 1080, 1920])\n",
      "Image size: torch.Size([3, 1080, 1920])\n",
      "Label size: torch.Size([4, 1080, 1920])\n",
      "Level segment size: torch.Size([1, 1080, 1920])\n",
      "Image size: torch.Size([3, 1080, 1920])\n",
      "Label size: torch.Size([4, 1080, 1920])\n",
      "Level segment size: torch.Size([1, 1080, 1920])\n",
      "Image size: torch.Size([3, 1080, 1920])\n",
      "Label size: torch.Size([4, 1080, 1920])\n",
      "Level segment size: torch.Size([1, 1080, 1920])\n",
      "Image size: torch.Size([3, 1080, 1920])\n",
      "Label size: torch.Size([4, 1080, 1920])\n",
      "Level segment size: torch.Size([1, 1080, 1920])\n",
      "Image size: torch.Size([3, 1080, 1920])\n",
      "Label size: torch.Size([4, 1080, 1920])\n",
      "Level segment size: torch.Size([1, 1080, 1920])\n",
      "Image size: torch.Size([3, 1080, 1920])\n",
      "Label size: torch.Size([4, 1080, 1920])\n",
      "Level segment size: torch.Size([1, 1080, 1920])\n",
      "Image size: torch.Size([3, 1080, 1920])\n",
      "Label size: torch.Size([4, 1080, 1920])\n",
      "Level segment size: torch.Size([1, 1080, 1920])\n",
      "Loaded batch with 32 images.\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-10T20:39:43.528310Z",
     "start_time": "2024-10-10T20:39:37.580869Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import helper\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "import os\n",
    "import json\n",
    "import segmentation_models_pytorch as smp\n",
    "import torchmetrics.functional as tmf\n",
    "from pytorch_lightning import LightningModule, Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "n_classes = 40  # Based on your labels\n",
    "\n",
    "# Split the dataset\n",
    "image_files = [os.path.join(dp, f) for dp, dn, filenames in os.walk(\"D:/New folder/Inter_Bootcamp/dataset/train\") for f in filenames if f.endswith('_leftImg8bit.jpg')]\n",
    "train_files, val_files = train_test_split(image_files, test_size=0.2, random_state=42)\n",
    "\n",
    "class CustomCityscapesDataset(Dataset):\n",
    "    def __init__(self, image_files, label_dir, transform=None):\n",
    "        self.image_files = image_files\n",
    "        self.label_dir = label_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_files[idx]\n",
    "        img_name = os.path.basename(img_path).replace('_leftImg8bit.jpg', '')\n",
    "        batch_name = os.path.basename(os.path.dirname(img_path))\n",
    "\n",
    "        label_path = os.path.join(self.label_dir, batch_name, f\"{img_name}_gtFine_labelColors.png\")\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        label = Image.open(label_path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            label = self.transform(label)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "# Function to encode segmented maps\n",
    "def encode_segmap(mask):\n",
    "    mask = mask.numpy() if isinstance(mask, torch.Tensor) else mask\n",
    "    mask = mask.astype(np.uint8)\n",
    "    label_map = np.zeros(mask.shape[:2], dtype=np.int32)\n",
    "    for label in labels:\n",
    "        color = np.array(label.color, dtype=np.uint8)\n",
    "        equal_mask = np.all(mask == color, axis=-1)\n",
    "        label_map[equal_mask] = label.level3Id\n",
    "    return label_map\n",
    "\n",
    "\n",
    "# Function to decode segmented maps\n",
    "def decode_segmap(label_mask, dataset='cityscapes', plot=False):\n",
    "    label_colours = {label.level3Id: label.color for label in labels}\n",
    "    r = label_mask.copy()\n",
    "    g = label_mask.copy()\n",
    "    b = label_mask.copy()\n",
    "    for l in np.unique(label_mask):\n",
    "        r[label_mask == l] = label_colours[l][0]\n",
    "        g[label_mask == l] = label_colours[l][1]\n",
    "        b[label_mask == l] = label_colours[l][2]\n",
    "    rgb = np.zeros((label_mask.shape[0], label_mask.shape[1], 3))\n",
    "    rgb[:, :, 0] = r / 255.0\n",
    "    rgb[:, :, 1] = g / 255.0\n",
    "    rgb[:, :, 2] = b / 255.0\n",
    "    return rgb\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = CustomCityscapesDataset(train_files, \"D:/New folder/Inter_Bootcamp/dataset/labels\", transform=transform)\n",
    "val_dataset = CustomCityscapesDataset(val_files, \"D:/New folder/Inter_Bootcamp/dataset/labels\", transform=transform)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n"
   ],
   "id": "c991f7a39b311c6a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prakh\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T02:50:42.179446Z",
     "start_time": "2024-10-11T02:50:41.330798Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "import os\n",
    "import json\n",
    "import segmentation_models_pytorch as smp\n",
    "import torchmetrics.functional as tmf\n",
    "from torchmetrics import JaccardIndex \n",
    "from pytorch_lightning import LightningModule, Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "n_classes = 40  # Based on your labels\n",
    "\n",
    "# Split the dataset\n",
    "image_files = [os.path.join(dp, f) for dp, dn, filenames in os.walk(\"D:/New folder/Inter_Bootcamp/dataset/train\") for f in filenames if f.endswith('_leftImg8bit.jpg')]\n",
    "train_files, val_files = train_test_split(image_files, test_size=0.2, random_state=42)\n",
    "\n",
    "class CustomCityscapesDataset(Dataset):\n",
    "    def __init__(self, image_files, label_dir, transform=None):\n",
    "        self.image_files = image_files\n",
    "        self.label_dir = label_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_files[idx]\n",
    "        img_name = os.path.basename(img_path).replace('_leftImg8bit.jpg', '')\n",
    "        batch_name = os.path.basename(os.path.dirname(img_path))\n",
    "\n",
    "        label_path = os.path.join(self.label_dir, batch_name, f\"{img_name}_gtFine_labelColors.png\")\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        label = Image.open(label_path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            label = self.transform(label)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "# Use the helper.py labels list directly\n",
    "labels_list = [\n",
    "    ('road', 0, 7, 0, 0, 0, 'drivable', 0, 0, False, False, (128, 64, 128)),\n",
    "    ('parking', 1, 9, 255, 1, 1, 'drivable', 1, 0, False, False, (250, 170, 160)),\n",
    "    ('drivable fallback', 2, 255, 255, 2, 1, 'drivable', 1, 0, False, False, (81, 0, 81)),\n",
    "    ('sidewalk', 3, 8, 1, 3, 2, 'non-drivable', 2, 1, False, False, (244, 35, 232)),\n",
    "    ('rail track', 4, 10, 255, 3, 3, 'non-drivable', 3, 1, False, False, (230, 150, 140)),\n",
    "    ('non-drivable fallback', 5, 255, 9, 4, 3, 'non-drivable', 3, 1, False, False, (152, 251, 152)),\n",
    "    ('person', 6, 24, 11, 5, 4, 'living-thing', 4, 2, True, False, (220, 20, 60)),\n",
    "    ('animal', 7, 255, 255, 6, 4, 'living-thing', 4, 2, True, True, (246, 198, 145)),\n",
    "    ('rider', 8, 25, 12, 7, 5, 'living-thing', 5, 2, True, False, (255, 0, 0)),\n",
    "    ('motorcycle', 9, 32, 17, 8, 6, '2-wheeler', 6, 3, True, False, (0, 0, 230)),\n",
    "    ('bicycle', 10, 33, 18, 9, 7, '2-wheeler', 6, 3, True, False, (119, 11, 32)),\n",
    "    ('autorickshaw', 11, 255, 255, 10, 8, 'autorickshaw', 7, 3, True, False, (255, 204, 54)),\n",
    "    ('car', 12, 26, 13, 11, 9, 'car', 7, 3, True, False, (0, 0, 142)),\n",
    "    ('truck', 13, 27, 14, 12, 10, 'large-vehicle', 8, 3, True, False, (0, 0, 70)),\n",
    "    ('bus', 14, 28, 15, 13, 11, 'large-vehicle', 8, 3, True, False, (0, 60, 100)),\n",
    "    ('caravan', 15, 29, 255, 14, 12, 'large-vehicle', 8, 3, True, True, (0, 0, 90)),\n",
    "    ('trailer', 16, 30, 255, 15, 12, 'large-vehicle', 8, 3, True, True, (0, 0, 110)),\n",
    "    ('train', 17, 31, 16, 15, 12, 'large-vehicle', 8, 3, True, True, (0, 80, 100)),\n",
    "    ('vehicle fallback', 18, 355, 255, 15, 12, 'large-vehicle', 8, 3, True, False, (136, 143, 153)),\n",
    "    ('curb', 19, 255, 255, 16, 13, 'barrier', 9, 4, False, False, (220, 190, 40)),\n",
    "    ('wall', 20, 12, 3, 17, 14, 'barrier', 9, 4, False, False, (102, 102, 156)),\n",
    "    ('fence', 21, 13, 4, 18, 15, 'barrier', 10, 4, False, False, (190, 153, 153)),\n",
    "    ('guard rail', 22, 14, 255, 19, 16, 'barrier', 10, 4, False, False, (180, 165, 180)),\n",
    "    ('billboard', 23, 255, 255, 20, 17, 'structures', 11, 4, False, False, (174, 64, 67)),\n",
    "    ('traffic sign', 24, 20, 7, 21, 18, 'structures', 11, 4, False, False, (220, 220, 0)),\n",
    "    ('traffic light', 25, 19, 6, 22, 19, 'structures', 11, 4, False, False, (250, 170, 30)),\n",
    "    ('pole', 26, 17, 5, 23, 20, 'structures', 12, 4, False, False, (153, 153, 153)),\n",
    "    ('polegroup', 27, 18, 255, 23, 20, 'structures', 12, 4, False, False, (153, 153, 153)),\n",
    "    ('obs-str-bar-fallback', 28, 255, 255, 24, 21, 'structures', 12, 4, False, False, (169, 187, 214)),\n",
    "    ('building', 29, 11, 2, 25, 22, 'construction', 13, 5, False, False, (70, 70, 70)),\n",
    "    ('bridge', 30, 15, 255, 26, 23, 'construction', 13, 5, False, False, (150, 100, 100)),\n",
    "    ('tunnel', 31, 16, 255, 26, 23, 'construction', 13, 5, False, False, (150, 120, 90)),\n",
    "    ('vegetation', 32, 21, 8, 27, 24, 'vegetation', 14, 5, False, False, (107, 142, 35)),\n",
    "    ('sky', 33, 23, 10, 28, 25, 'sky', 15, 6, False, False, (70, 130, 180)),\n",
    "    ('fallback background', 34, 255, 255, 29, 25, 'object fallback', 15, 6, False, False, (169, 187, 214)),\n",
    "    ('unlabeled', 35, 0, 255, 255, 255, 'void', 255, 255, False, True, (0, 0, 0)),\n",
    "    ('ego vehicle', 36, 1, 255, 255, 255, 'void', 255, 255, False, True, (0, 0, 0)),\n",
    "    ('rectification border', 37, 2, 255, 255, 255, 'void', 255, 255, False, True, (0, 0, 0)),\n",
    "    ('out of roi', 38, 3, 255, 255, 255, 'void', 255, 255, False, True, (0, 0, 0)),\n",
    "    ('license plate', 39, 255, 255, 255, 255, 'vehicle', 255, 255, False, True, (0, 0, 142)),\n",
    "]\n",
    "\n",
    "# Function to encode segmented maps\n",
    "def encode_segmap(mask, label_list):\n",
    "    mask = mask.astype(np.uint8)\n",
    "    label_map = np.zeros(mask.shape[:2], dtype=np.int32)\n",
    "    for label in label_list:\n",
    "        r, g, b = label[11]\n",
    "        color_mask = (mask[:, :, 0] == r) & (mask[:, :, 1] == g) & (mask[:, :, 2] == b)\n",
    "        label_map[color_mask] = label[5]\n",
    "    return label_map\n",
    "\n",
    "class OurModel(LightningModule):\n",
    "    def __init__(self):\n",
    "        super(OurModel, self).__init__()\n",
    "        self.model = smp.Unet(\n",
    "            encoder_name=\"resnet50\", \n",
    "            encoder_weights=\"imagenet\", \n",
    "            in_channels=3, \n",
    "            classes=n_classes\n",
    "        )\n",
    "        self.lr = 1e-3\n",
    "        self.criterion = smp.losses.DiceLoss(mode='multiclass')\n",
    "        self.metric = JaccardIndex(task='multiclass',num_classes=n_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(self.parameters(), lr=self.lr)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        img, seg = batch\n",
    "    \n",
    "        # Move img and seg to the device (GPU) and set the correct data types\n",
    "        img = img.to('cuda').half()  # Convert input images to float16 and move to GPU\n",
    "        seg = seg.permute(0, 2, 3, 1).cpu().numpy()  # BCHW -> BHWC\n",
    "        seg = np.array([encode_segmap(s, labels_list) for s in seg])  # Encode segmentation mask\n",
    "        seg = torch.tensor(seg, dtype=torch.long).to('cuda')  # Convert to tensor and move to GPU\n",
    "    \n",
    "        # Forward pass\n",
    "        output = self(img)  # Output shape: [batch_size, n_classes, height, width]\n",
    "        \n",
    "        print(f\"Output shape: {output.shape}, Segmentation shape: {seg.shape}\")\n",
    "        # Convert the output to class indices\n",
    "        pred = torch.argmax(output, dim=1)  # Get the predicted class index for each pixel\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = self.criterion(output, seg)\n",
    "        \n",
    "        # Calculate IoU (Jaccard Index)\n",
    "        iou = self.metric(pred, seg)\n",
    "        \n",
    "        # Log metrics\n",
    "        self.log('train_loss', loss, on_epoch=True)\n",
    "        self.log('train_iou', iou, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        img, seg = batch\n",
    "    \n",
    "        # Move img and seg to the device (GPU) and set the correct data types\n",
    "        img = img.to('cuda').half()  # Convert input images to float16 and move to GPU\n",
    "        seg = seg.permute(0, 2, 3, 1).cpu().numpy()  # BCHW -> BHWC\n",
    "        seg = np.array([encode_segmap(s, labels_list) for s in seg])  # Encode segmentation mask\n",
    "        seg = torch.tensor(seg, dtype=torch.long).to('cuda')  # Convert to tensor and move to GPU\n",
    "    \n",
    "        # Forward pass\n",
    "        output = self(img)  # Output shape: [batch_size, n_classes, height, width]\n",
    "    \n",
    "        print(f\"Output shape: {output.shape}, Segmentation shape: {seg.shape}\")\n",
    "\n",
    "        # Convert the output to class indices\n",
    "        pred = torch.argmax(output, dim=1)  # Get the predicted class index for each pixel\n",
    "    \n",
    "        # Calculate loss\n",
    "        loss = self.criterion(output, seg)\n",
    "        \n",
    "        # Calculate IoU (Jaccard Index)\n",
    "        iou = self.metric(pred, seg)\n",
    "    \n",
    "        # Log metrics\n",
    "        self.log('val_loss', loss, on_epoch=True)\n",
    "        self.log('val_iou', iou, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "# Training setup remains the same\n",
    "checkpoint_callback = ModelCheckpoint(monitor='val_loss', save_top_k=1)\n",
    "trainer = Trainer(\n",
    "    max_epochs=30,\n",
    "    accelerator='gpu', \n",
    "    precision=32, \n",
    "    callbacks=[checkpoint_callback]\n",
    ")\n",
    "model = OurModel()\n",
    "trainer.fit(model, train_loader, val_loader)"
   ],
   "id": "817d7ac5395dde9b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "File \u001B[1;32m~\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py:47\u001B[0m, in \u001B[0;36m_call_and_handle_interrupt\u001B[1;34m(trainer, trainer_fn, *args, **kwargs)\u001B[0m\n\u001B[0;32m     46\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m trainer\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39mlauncher\u001B[38;5;241m.\u001B[39mlaunch(trainer_fn, \u001B[38;5;241m*\u001B[39margs, trainer\u001B[38;5;241m=\u001B[39mtrainer, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m---> 47\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m trainer_fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m     49\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m _TunerExitException:\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:574\u001B[0m, in \u001B[0;36mTrainer._fit_impl\u001B[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001B[0m\n\u001B[0;32m    568\u001B[0m ckpt_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_checkpoint_connector\u001B[38;5;241m.\u001B[39m_select_ckpt_path(\n\u001B[0;32m    569\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mfn,\n\u001B[0;32m    570\u001B[0m     ckpt_path,\n\u001B[0;32m    571\u001B[0m     model_provided\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[0;32m    572\u001B[0m     model_connected\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlightning_module \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    573\u001B[0m )\n\u001B[1;32m--> 574\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_run(model, ckpt_path\u001B[38;5;241m=\u001B[39mckpt_path)\n\u001B[0;32m    576\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mstopped\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:957\u001B[0m, in \u001B[0;36mTrainer._run\u001B[1;34m(self, model, ckpt_path)\u001B[0m\n\u001B[0;32m    956\u001B[0m \u001B[38;5;66;03m# strategy will configure model and move it to the device\u001B[39;00m\n\u001B[1;32m--> 957\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39msetup(\u001B[38;5;28mself\u001B[39m)\n\u001B[0;32m    959\u001B[0m \u001B[38;5;66;03m# hook\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\pytorch_lightning\\strategies\\strategy.py:148\u001B[0m, in \u001B[0;36mStrategy.setup\u001B[1;34m(self, trainer)\u001B[0m\n\u001B[0;32m    147\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maccelerator \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m--> 148\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maccelerator\u001B[38;5;241m.\u001B[39msetup(trainer)\n\u001B[0;32m    150\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\pytorch_lightning\\accelerators\\cuda.py:53\u001B[0m, in \u001B[0;36mCUDAAccelerator.setup\u001B[1;34m(self, trainer)\u001B[0m\n\u001B[0;32m     52\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mset_nvidia_flags(trainer\u001B[38;5;241m.\u001B[39mlocal_rank)\n\u001B[1;32m---> 53\u001B[0m _clear_cuda_memory()\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\lightning_fabric\\accelerators\\cuda.py:181\u001B[0m, in \u001B[0;36m_clear_cuda_memory\u001B[1;34m()\u001B[0m\n\u001B[0;32m    180\u001B[0m     torch\u001B[38;5;241m.\u001B[39m_C\u001B[38;5;241m.\u001B[39m_cuda_clearCublasWorkspaces()\n\u001B[1;32m--> 181\u001B[0m torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39mempty_cache()\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\torch\\cuda\\memory.py:170\u001B[0m, in \u001B[0;36mempty_cache\u001B[1;34m()\u001B[0m\n\u001B[0;32m    169\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_initialized():\n\u001B[1;32m--> 170\u001B[0m     torch\u001B[38;5;241m.\u001B[39m_C\u001B[38;5;241m.\u001B[39m_cuda_emptyCache()\n",
      "\u001B[1;31mRuntimeError\u001B[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[29], line 181\u001B[0m\n\u001B[0;32m    174\u001B[0m trainer \u001B[38;5;241m=\u001B[39m Trainer(\n\u001B[0;32m    175\u001B[0m     max_epochs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m30\u001B[39m,\n\u001B[0;32m    176\u001B[0m     accelerator\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mgpu\u001B[39m\u001B[38;5;124m'\u001B[39m, \n\u001B[0;32m    177\u001B[0m     precision\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m32\u001B[39m, \n\u001B[0;32m    178\u001B[0m     callbacks\u001B[38;5;241m=\u001B[39m[checkpoint_callback]\n\u001B[0;32m    179\u001B[0m )\n\u001B[0;32m    180\u001B[0m model \u001B[38;5;241m=\u001B[39m OurModel()\n\u001B[1;32m--> 181\u001B[0m trainer\u001B[38;5;241m.\u001B[39mfit(model, train_loader, val_loader)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:538\u001B[0m, in \u001B[0;36mTrainer.fit\u001B[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001B[0m\n\u001B[0;32m    536\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mstatus \u001B[38;5;241m=\u001B[39m TrainerStatus\u001B[38;5;241m.\u001B[39mRUNNING\n\u001B[0;32m    537\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m--> 538\u001B[0m call\u001B[38;5;241m.\u001B[39m_call_and_handle_interrupt(\n\u001B[0;32m    539\u001B[0m     \u001B[38;5;28mself\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001B[0;32m    540\u001B[0m )\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py:68\u001B[0m, in \u001B[0;36m_call_and_handle_interrupt\u001B[1;34m(trainer, trainer_fn, *args, **kwargs)\u001B[0m\n\u001B[0;32m     66\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exception:\n\u001B[0;32m     67\u001B[0m     _interrupt(trainer, exception)\n\u001B[1;32m---> 68\u001B[0m     trainer\u001B[38;5;241m.\u001B[39m_teardown()\n\u001B[0;32m     69\u001B[0m     \u001B[38;5;66;03m# teardown might access the stage so we reset it after\u001B[39;00m\n\u001B[0;32m     70\u001B[0m     trainer\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mstage \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1004\u001B[0m, in \u001B[0;36mTrainer._teardown\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1001\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_teardown\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   1002\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"This is the Trainer's internal teardown, unrelated to the `teardown` hooks in LightningModule and Callback;\u001B[39;00m\n\u001B[0;32m   1003\u001B[0m \u001B[38;5;124;03m    those are handled by :meth:`_call_teardown_hook`.\"\"\"\u001B[39;00m\n\u001B[1;32m-> 1004\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39mteardown()\n\u001B[0;32m   1005\u001B[0m     loop \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_active_loop\n\u001B[0;32m   1006\u001B[0m     \u001B[38;5;66;03m# loop should never be `None` here but it can because we don't know the trainer stage with `ddp_spawn`\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\pytorch_lightning\\strategies\\strategy.py:538\u001B[0m, in \u001B[0;36mStrategy.teardown\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    536\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprecision_plugin\u001B[38;5;241m.\u001B[39mteardown()\n\u001B[0;32m    537\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maccelerator \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m--> 538\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maccelerator\u001B[38;5;241m.\u001B[39mteardown()\n\u001B[0;32m    539\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcheckpoint_io\u001B[38;5;241m.\u001B[39mteardown()\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\pytorch_lightning\\accelerators\\cuda.py:82\u001B[0m, in \u001B[0;36mCUDAAccelerator.teardown\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     80\u001B[0m \u001B[38;5;129m@override\u001B[39m\n\u001B[0;32m     81\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mteardown\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m---> 82\u001B[0m     _clear_cuda_memory()\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\lightning_fabric\\accelerators\\cuda.py:181\u001B[0m, in \u001B[0;36m_clear_cuda_memory\u001B[1;34m()\u001B[0m\n\u001B[0;32m    178\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(torch\u001B[38;5;241m.\u001B[39m_C, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_cuda_clearCublasWorkspaces\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m    179\u001B[0m     \u001B[38;5;66;03m# https://github.com/pytorch/pytorch/issues/95668\u001B[39;00m\n\u001B[0;32m    180\u001B[0m     torch\u001B[38;5;241m.\u001B[39m_C\u001B[38;5;241m.\u001B[39m_cuda_clearCublasWorkspaces()\n\u001B[1;32m--> 181\u001B[0m torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39mempty_cache()\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\torch\\cuda\\memory.py:170\u001B[0m, in \u001B[0;36mempty_cache\u001B[1;34m()\u001B[0m\n\u001B[0;32m    159\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"Release all unoccupied cached memory currently held by the caching\u001B[39;00m\n\u001B[0;32m    160\u001B[0m \u001B[38;5;124;03mallocator so that those can be used in other GPU application and visible in\u001B[39;00m\n\u001B[0;32m    161\u001B[0m \u001B[38;5;124;03m`nvidia-smi`.\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    167\u001B[0m \u001B[38;5;124;03m    more details about GPU memory management.\u001B[39;00m\n\u001B[0;32m    168\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    169\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_initialized():\n\u001B[1;32m--> 170\u001B[0m     torch\u001B[38;5;241m.\u001B[39m_C\u001B[38;5;241m.\u001B[39m_cuda_emptyCache()\n",
      "\u001B[1;31mRuntimeError\u001B[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Evaluate the model\n",
    "trainer.validate(model, val_loader)\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), 'trained_model_2.pth')\n",
    "print(\"Model 2 saved successfully.\")"
   ],
   "id": "5a8881eaa4a33d16"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model = OurModel()\n",
    "model.load_state_dict(torch.load('trained_model_2.pth'))\n",
    "model.eval()"
   ],
   "id": "2ade0c1c4e948f89"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Define the Label class and the list of labels\n",
    "class Label:\n",
    "    def __init__(self, name, id, csId, csTrainId, level4id, level3Id, category, level2Id, level1Id, hasInstances, ignoreInEval, color):\n",
    "        self.name = name\n",
    "        self.id = id\n",
    "        self.csId = csId\n",
    "        self.csTrainId = csTrainId\n",
    "        self.level4id = level4id\n",
    "        self.level3Id = level3Id\n",
    "        self.category = category\n",
    "        self.level2Id = level2Id\n",
    "        self.level1Id = level1Id\n",
    "        self.hasInstances = hasInstances\n",
    "        self.ignoreInEval = ignoreInEval\n",
    "        self.color = color\n",
    "\n",
    "# Your list of labels (as previously defined)\n",
    "labels = [\n",
    "    Label('road', 0, 7, 0, 0, 0, 'drivable', 0, 0, False, False, (128, 64, 128)),\n",
    "    Label('parking', 1, 9, 255, 1, 1, 'drivable', 1, 0, False, False, (250, 170, 160)),\n",
    "    Label('drivable fallback', 2, 255, 255, 2, 1, 'drivable', 1, 0, False, False, (81, 0, 81)),\n",
    "    Label('sidewalk', 3, 8, 1, 3, 2, 'non-drivable', 2, 1, False, False, (244, 35, 232)),\n",
    "    Label('rail track', 4, 10, 255, 3, 3, 'non-drivable', 3, 1, False, False, (230, 150, 140)),\n",
    "    Label('non-drivable fallback', 5, 255, 9, 4, 3, 'non-drivable', 3, 1, False, False, (152, 251, 152)),\n",
    "    Label('person', 6, 24, 11, 5, 4, 'living-thing', 4, 2, True, False, (220, 20, 60)),\n",
    "    Label('animal', 7, 255, 255, 6, 4, 'living-thing', 4, 2, True, True, (246, 198, 145)),\n",
    "    Label('rider', 8, 25, 12, 7, 5, 'living-thing', 5, 2, True, False, (255, 0, 0)),\n",
    "    Label('motorcycle', 9, 32, 17, 8, 6, '2-wheeler', 6, 3, True, False, (0, 0, 230)),\n",
    "    Label('bicycle', 10, 33, 18, 9, 7, '2-wheeler', 6, 3, True, False, (119, 11, 32)),\n",
    "    Label('autorickshaw', 11, 255, 255, 10, 8, 'autorickshaw', 7, 3, True, False, (255, 204, 54)),\n",
    "    Label('car', 12, 26, 13, 11, 9, 'car', 7, 3, True, False, (0, 0, 142)),\n",
    "    Label('truck', 13, 27, 14, 12, 10, 'large-vehicle', 8, 3, True, False, (0, 0, 70)),\n",
    "    Label('bus', 14, 28, 15, 13, 11, 'large-vehicle', 8, 3, True, False, (0, 60, 100)),\n",
    "    Label('caravan', 15, 29, 255, 14, 12, 'large-vehicle', 8, 3, True, True, (0, 0, 90)),\n",
    "    Label('trailer', 16, 30, 255, 15, 12, 'large-vehicle', 8, 3, True, True, (0, 0, 110)),\n",
    "    Label('train', 17, 31, 16, 15, 12, 'large-vehicle', 8, 3, True, True, (0, 80, 100)),\n",
    "    Label('vehicle fallback', 18, 355, 255, 15, 12, 'large-vehicle', 8, 3, True, False, (136, 143, 153)),\n",
    "    Label('curb', 19, 255, 255, 16, 13, 'barrier', 9, 4, False, False, (220, 190, 40)),\n",
    "    Label('wall', 20, 12, 3, 17, 14, 'barrier', 9, 4, False, False, (102, 102, 156)),\n",
    "    Label('fence', 21, 13, 4, 18, 15, 'barrier', 10, 4, False, False, (190, 153, 153)),\n",
    "    Label('guard rail', 22, 14, 255, 19, 16, 'barrier', 10, 4, False, False, (180, 165, 180)),\n",
    "    Label('billboard', 23, 255, 255, 20, 17, 'structures', 11, 4, False, False, (174, 64, 67)),\n",
    "    Label('traffic sign', 24, 20, 7, 21, 18, 'structures', 11, 4, False, False, (220, 220, 0)),\n",
    "    Label('traffic light', 25, 19, 6, 22, 19, 'structures', 11, 4, False, False, (250, 170, 30)),\n",
    "    Label('pole', 26, 17, 5, 23, 20, 'structures', 12, 4, False, False, (153, 153, 153)),\n",
    "    Label('polegroup', 27, 18, 255, 23, 20, 'structures', 12, 4, False, False, (153, 153, 153)),\n",
    "    Label('obs-str-bar-fallback', 28, 255, 255, 24, 21, 'structures', 12, 4, False, False, (169, 187, 214)),\n",
    "    Label('building', 29, 11, 2, 25, 22, 'construction', 13, 5, False, False, (70, 70, 70)),\n",
    "    Label('bridge', 30, 15, 255, 26, 23, 'construction', 13, 5, False, False, (150, 100, 100)),\n",
    "    Label('tunnel', 31, 16, 255, 26, 23, 'construction', 13, 5, False, False, (150, 120, 90)),\n",
    "    Label('vegetation', 32, 21, 8, 27, 24, 'vegetation', 14, 5, False, False, (107, 142, 35)),\n",
    "    Label('sky', 33, 23, 10, 28, 25, 'sky', 15, 6, False, False, (70, 130, 180)),\n",
    "    Label('fallback background', 34, 255, 255, 29, 25, 'object fallback', 15, 6, False, False, (169, 187, 214)),\n",
    "    Label('unlabeled', 35, 0, 255, 255, 255, 'void', 255, 255, False, True, (0, 0, 0)),\n",
    "    Label('ego vehicle', 36, 1, 255, 255, 255, 'void', 255, 255, False, True, (0, 0, 0)),\n",
    "    Label('rectification border', 37, 2, 255, 255, 255, 'void', 255, 255, False, True, (0, 0, 0)),\n",
    "    Label('out of roi', 38, 3, 255, 255, 255, 'void', 255, 255, False, True, (0, 0, 0)),\n",
    "    Label('license plate', 39, 255, 255, 255, 255, 'vehicle', 255, 255, False, True, (0, 0, 142)),\n",
    "]\n",
    "\n",
    "# Custom dataset for test set\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, image_files, transform=None):\n",
    "        self.image_files = image_files\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_files[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, img_path\n",
    "\n",
    "# Function to pad image to be divisible by 32\n",
    "def pad_image(img):\n",
    "    c, h, w = img.shape  # Correct shape unpacking\n",
    "    new_h = ((h // 32) + 1) * 32 if h % 32 != 0 else h\n",
    "    new_w = ((w // 32) + 1) * 32 if w % 32 != 0 else w\n",
    "    padded_img = torch.zeros((c, new_h, new_w))\n",
    "    padded_img[:, :h, :w] = img\n",
    "    return padded_img\n",
    "\n",
    "# Function to convert the segmented image into polygon-based format and prepare for CSV\n",
    "def image_to_polygon_format(segmented_image):\n",
    "    # Assuming segmented_image is of shape (height, width, num_classes)\n",
    "    height, width, num_classes = segmented_image.shape\n",
    "    objects = []  # To hold the polygon data for each object\n",
    "\n",
    "    for label_idx, label in enumerate(labels):\n",
    "        if label.ignoreInEval:  # Skip labels that should be ignored in evaluation\n",
    "            continue\n",
    "\n",
    "        # Extract the specific class channel\n",
    "        class_channel = segmented_image[:, :, label_idx]\n",
    "\n",
    "        # Threshold to create binary mask\n",
    "        mask = (class_channel > 0.5).astype(np.uint8) * 255  # Convert to 0-255 mask\n",
    "\n",
    "        # Find contours (polygons) in the binary mask\n",
    "        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "        for contour in contours:\n",
    "            if len(contour) < 3:  # Skip small or invalid polygons\n",
    "                continue\n",
    "\n",
    "            # Simplify the contour to polygon format\n",
    "            polygon = contour.reshape(-1, 2).tolist()  # Flatten the contour to a list of points\n",
    "\n",
    "            # Create the object data for the current label and polygon\n",
    "            object_data = {\n",
    "                \"label\": label.name,  # Use the label's name\n",
    "                \"polygon\": polygon\n",
    "            }\n",
    "\n",
    "            # Append the object to the result\n",
    "            objects.append(object_data)\n",
    "\n",
    "    return objects\n",
    "\n",
    "# Function to save to the CSV format as requested\n",
    "def save_to_csv(objects_dict, output_csv_path):\n",
    "    # Create a DataFrame with id and objects\n",
    "    data = [{\"id\": filename, \"objects\": json.dumps(objects)} for filename, objects in objects_dict.items()]\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Save to CSV\n",
    "    df.to_csv(output_csv_path, index=False)\n",
    "\n",
    "# Function to run inference\n",
    "def run_inference(model, test_loader):\n",
    "    model.eval()\n",
    "    results = {}\n",
    "    for img, img_path in test_loader:\n",
    "        img = pad_image(img.squeeze(0))  # Pad the image and remove batch dimension\n",
    "        filename = os.path.basename(img_path[0]).replace('_leftImg8bit.jpg', '')\n",
    "        output = model(img.unsqueeze(0)).detach().cpu().numpy()\n",
    "        segmented_image = output[0].transpose(1, 2, 0)  # Convert to HWC\n",
    "        objects = image_to_polygon_format(segmented_image)  # Convert to polygons\n",
    "        results[filename] = objects\n",
    "    return results\n",
    "\n",
    "\n",
    "# Load your trained model\n",
    "model = OurModel()\n",
    "model.load_state_dict(torch.load('trained_model_2.pth'))\n",
    "model.eval()\n",
    "\n",
    "# Create a DataLoader for your test set\n",
    "test_files = [os.path.join(\"D:/New folder/Inter_Bootcamp/dataset/test\", f) for f in os.listdir(\"D:/New folder/Inter_Bootcamp/dataset/test\") if f.endswith('_leftImg8bit.jpg')]\n",
    "test_dataset = TestDataset(test_files, transform=transforms.ToTensor())\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# Run inference\n",
    "results = run_inference(model, test_loader)\n",
    "\n",
    "# Save results to submission file\n",
    "save_to_csv(results, 'submission3.csv')\n",
    "print(\"Submission 3 file created successfully.\")\n"
   ],
   "id": "d8dca32a39fb27fd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-10T21:02:03.200928Z",
     "start_time": "2024-10-10T21:02:03.194906Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())  # Should return True if a GPU is available\n"
   ],
   "id": "887ef4f3b4fe7a5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-10T21:03:19.766031Z",
     "start_time": "2024-10-10T21:03:19.761151Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.get_device_name(0))\n"
   ],
   "id": "43cdbe1e6d6dd3f8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n",
      "NVIDIA GeForce RTX 3050 Laptop GPU\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-10T21:03:38.130425Z",
     "start_time": "2024-10-10T21:03:38.126646Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pytorch_lightning as pl\n",
    "print(pl.__version__)\n"
   ],
   "id": "51a17487556397af",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.0\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-10T21:04:11.593622Z",
     "start_time": "2024-10-10T21:04:06.085334Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install pytorch-lightning==2.0.0",
   "id": "6723cfd16227501e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorch-lightning==2.0.0\n",
      "  Downloading pytorch_lightning-2.0.0-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: numpy>=1.17.2 in c:\\users\\prakh\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from pytorch-lightning==2.0.0) (1.26.4)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\prakh\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from pytorch-lightning==2.0.0) (2.4.1+cu118)\n",
      "Requirement already satisfied: tqdm>=4.57.0 in c:\\users\\prakh\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from pytorch-lightning==2.0.0) (4.66.5)\n",
      "Requirement already satisfied: PyYAML>=5.4 in c:\\users\\prakh\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from pytorch-lightning==2.0.0) (6.0.1)\n",
      "Requirement already satisfied: fsspec>2021.06.0 in c:\\users\\prakh\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from fsspec[http]>2021.06.0->pytorch-lightning==2.0.0) (2024.9.0)\n",
      "Requirement already satisfied: torchmetrics>=0.7.0 in c:\\users\\prakh\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from pytorch-lightning==2.0.0) (1.4.2)\n",
      "Requirement already satisfied: packaging>=17.1 in c:\\users\\prakh\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from pytorch-lightning==2.0.0) (24.1)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\prakh\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from pytorch-lightning==2.0.0) (4.11.0)\n",
      "Requirement already satisfied: lightning-utilities>=0.7.0 in c:\\users\\prakh\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from pytorch-lightning==2.0.0) (0.11.7)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\prakh\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from fsspec[http]>2021.06.0->pytorch-lightning==2.0.0) (3.10.9)\n",
      "Requirement already satisfied: setuptools in c:\\users\\prakh\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from lightning-utilities>=0.7.0->pytorch-lightning==2.0.0) (75.1.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\prakh\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from torch>=1.11.0->pytorch-lightning==2.0.0) (3.13.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\prakh\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from torch>=1.11.0->pytorch-lightning==2.0.0) (1.13.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\prakh\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from torch>=1.11.0->pytorch-lightning==2.0.0) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\prakh\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from torch>=1.11.0->pytorch-lightning==2.0.0) (3.1.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\prakh\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from tqdm>=4.57.0->pytorch-lightning==2.0.0) (0.4.6)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\prakh\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning==2.0.0) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\prakh\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning==2.0.0) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\prakh\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning==2.0.0) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\prakh\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning==2.0.0) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\prakh\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning==2.0.0) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in c:\\users\\prakh\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning==2.0.0) (1.14.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\prakh\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from jinja2->torch>=1.11.0->pytorch-lightning==2.0.0) (2.1.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\prakh\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from sympy->torch>=1.11.0->pytorch-lightning==2.0.0) (1.3.0)\n",
      "Requirement already satisfied: idna>=2.0 in c:\\users\\prakh\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from yarl<2.0,>=1.12.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning==2.0.0) (3.7)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\prakh\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from yarl<2.0,>=1.12.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning==2.0.0) (0.2.0)\n",
      "Downloading pytorch_lightning-2.0.0-py3-none-any.whl (715 kB)\n",
      "   ---------------------------------------- 0.0/715.6 kB ? eta -:--:--\n",
      "   ----------------------------- ---------- 524.3/715.6 kB 3.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 715.6/715.6 kB 2.9 MB/s eta 0:00:00\n",
      "Installing collected packages: pytorch-lightning\n",
      "  Attempting uninstall: pytorch-lightning\n",
      "    Found existing installation: pytorch-lightning 2.4.0\n",
      "    Uninstalling pytorch-lightning-2.4.0:\n",
      "      Successfully uninstalled pytorch-lightning-2.4.0\n",
      "Successfully installed pytorch-lightning-2.0.0\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "30ea3649cc509db3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
